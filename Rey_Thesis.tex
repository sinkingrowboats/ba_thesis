\documentclass[psamsfonts, 10pt]{amsart}

%-------Packages---------
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multirow, changepage}
\usepackage{bbm}
\usepackage[ruled, linesnumbered, longend]{algorithm2e}
\usepackage{amssymb,amsfonts}
\usepackage[all,arc]{xy}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}


%------- Lowercase script packages---------

\makeatletter
\DeclareFontEncoding{LS1}{}{}
\DeclareFontSubstitution{LS1}{stix}{m}{n}
\DeclareMathAlphabet{\mathscr}{LS1}{stixscr}{m}{n}
\makeatother

\newcommand{\textscr}[1]{%
  \text{\usefont{LS1}{stixscr}{m}{n}#1}%
}


%---------Graphics Paths----------
\graphicspath{ {images/} }

%--------Theorem Environments--------
%theoremstyle{plain} --- default
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{quest}[thm]{Question}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{defns}[thm]{Definitions}
\newtheorem{con}[thm]{Construction}
\newtheorem{exmp}[thm]{Example}
\newtheorem{exmps}[thm]{Examples}
\newtheorem{notn}[thm]{Notation}
\newtheorem{notns}[thm]{Notations}
\newtheorem{addm}[thm]{Addendum}
\newtheorem{exer}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{warn}[thm]{Warning}
\newtheorem{sch}[thm]{Scholium}

\makeatletter
\let\c@equation\c@thm
\makeatother
\numberwithin{equation}{section}

\bibliographystyle{plain}


%---------------My Commands---------------
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{claim}[2][Claim]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]:}{\end{trivlist}}
\newenvironment{subclaim}[2][Sub Claim]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2}]:}{\end{trivlist}}
\newenvironment{term}[1]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1 -}]}{\end{trivlist}}



\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Epsilon}{\mathcal{E}}
\newcommand{\tor}{\text{ or } }
\newcommand{\tand}{\text{ and }}
\newcommand{\cross}{\ \times \ }
\newcommand{\creturn}{\mbox{}\\}
\newcommand{\todo}{\text{ todo. }}
\newcommand{\plus}{ \ + \ }
\newcommand{\minus}{ \ - \ }
\newcommand{\tiff}{{\it iff} }
\newcommand{\timplies}{{\it implies}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\SetKwRepeat{Struct}{struct \{}{\}}%

%--------Meta Data: Fill in your info------
\title{An Introductory Survey of Randomized Graph Algorithms}

\author{Samantha Rey}

\date{5/14/2018}

\begin{document}

\begin{abstract}

Randomized algorithms are noteworthy for their ability to present simple solutions that are efficient with high probability and can withstand adversarial input. This paper serves as an exploratory introduction to the topic of randomized algorithms, focusing specifically on graph algorithms. We introduce three different randomized graph algorithms and analyze their expected running times. We begin with an algorithm that specifies randomized routing protocol for a computer network, continuing on to an analysis Karger's algorithm and it's uses for both finding and counting global-minimum cuts, finishing with an algorithm for determining graph connectivity.

\end{abstract}

\maketitle

\tableofcontents













\section{Randomized Routing} In this section, we present an oblivious randomized routing protocol for a network of $N = 2^n$ processors in an $n$-dimensional boolean hypercube that aims to solve the {\it permuatation routing problem} \cite{randalg}.  A routing protocol is considered ``oblivious'' when the algorithm chooses routes independently for each processor, where each processor $i$'s route depends only on it's own destination $d(i)$ and does not depend on the destination $d(j)$ of any other processor $j$. Deterministic oblivious routing protocols are weak to adversarial inputs, with there being an instance of the problem that always requires $\Omega\Big(\sqrt{(N/n)}\Big)$ steps to complete. However, because the routing protocol we present is randomized, on any input, our algorithm resolves in $O(n)$ steps with high probability and is provably superior under conditions of adversarial input. Presentation of this problem adapts from section 4.2 of \cite{randalg}. \\

For this problem, we take the following model of a parallel computer 
\begin{adjustwidth}{3em}{3em}
We model a network of $N$ parallel processors in a computer as a directed graph on $N$ nodes. Each node represents a single processor, and possesses a unique identifying number between 0 and $N-1$, and a queue from which the processor sends out packets. The processors communicate with each other by sending information in the form of ``packets'' over their communication links. Each edge in the graph represents a communication link in the computer. That is, an edge from processor $i$ to $j$ represents a communication link by which processor $i$ can send packets to processor $j$. When routing packets, the computer adheres to the following:
\begin{enumerate}
  \item All communication between processors proceeds in a sequence of synchronous steps, which can be represented by a sequence of numbers all between $0$ and $N-1$ that shows in order each processor a packet visits as it travels to its destination.
  \item Each link can carry at most 1 packet in a step.
  \item During a step, a processor can send at most one packet to each of its neighbors.
  \item As packets arrive at a processor, the processor places them in its FIFO ordered queue in the order which they arrived
  \item During each step, for each communication link attached to our processor $i$ with destination processor $j$, if there is any packet in the queue to be routed next in its sequence to $j$, then $i$ transmits that packet to $j$ and removes the packet transmitted from its queue.
  \item If for a certain communication link attached to $i$ with destination $j$, there exists more than one packet in the queue of $i$ to be transmitted next to $j$, then processor $i$ removes only the first packet in the queue and transmits that packet.
\end{enumerate}
\end{adjustwidth}

For the purpose of our analysis, we will be using a $n$ dimensional hypercube to model the processor network of our computer, such that $N = 2^n$. Therefore, each processor's identifying number can be represented by an $n$ bit binary string. In the hyper cube, a communication link exists between all processor nodes that differ by one bit in the binary string representation of their id number. Each processor is connected to $n$ other processors in the cube. For example, in the 3-dimensional hypercube, processor 2 has 3-bit binary id 010. Processor 2 is therefore connected to processors 0, 3, and 6 because the 3-bit binary ids of these processors are 000, 011, and 110 respectively, which all differ from 010 by one exactly one bit in their binary string representation. \\

Now, we consider the {\it permutation routing problem} on our model \cite{randalg}. Each processor has a packet to be delivered to some processor in the network. Let $v_i$ denote the packet originating at processor $i$ and let $d(i)$ denote the the destination processor for packet $v_i$. In the permutation routing problem, for all $0 \leq i \leq N-1$, the $d(i)$s form a a permutation of $\{0, ... , N-1\}$. That is, each of the $N$ packets routes to a unique destination processor and each of the processors is the destination of exactly one packet. For the permutation routing problem, we seek an algorithm that can route any arbitrary permutation of destination processors. The algorithm must specify a route for each packet, with the goal of minimizing the number of steps required for all packets to reach their destination.\\

Our proposed solution to the permutation routing problem is the following {\bf Randomized Routing} algorithm. The first subroutine of which is:
\\ \\

\begin{algorithm}[H]
\KwIn{Current processor $i$, Packet $v_i$, Destination processor $j$}
\If{$i = j$} {
	{\bf End}\;
}
$k = 1$\;
\While{$i[k] = j[k]$} {
	$k$++\;
}
$i' =$ concat($i[1:k], !i[k], i[k+1: n+1]$)\;
send $v_i$ to $i'$\;
{\bf bitFixRoute}($i', v_i, j$)\;
\caption{{\bf bitFixRoute}($i$, $v_i$, $j$)}
\end{algorithm}
\creturn

Let $i$ and $j$ be two processors ids in our $N$ node processor network. Suppose we were to route a packet $v_i$ from $i$ to $j$ using the {\bf bitFixRoute}($i, v_i, j$). Let the $k$th bit be the lowest indexed bit on which the binary strings $i$ and $j$ differ. We send the packet down the communication link to the processor $i'$ with the same binary id representation as $i$, except the $k$th bit is flipped. To this end, the binary id of $i'$ is the concatenation $i[1:k]$, $!i[k]$, and $i[k+1: n+1]$, where $i[\ell, \ell']$ is the substring from the $\ell^{th}$ character to the $\ell' -1^{st}$ character of the binary string representation of $i$ and $!i[\ell]$ is the $\ell^{th}$ of the string representation of $i$ flipped. Now, we send $v_i$ to processor $i'$, let $i = i'$, and repeat the process until $i = j$. So, packet $v_i$ will eventually reach $j$ from $i$ using this bit fixing strategy. To this end, the subroutine {\bf bitFixRoute} moves a packet $v_i$ from its starting processor $i$ to it's destination processor $j$ by changing the bits of $i$ from left to right as needed to match $j$, where each movement of $v_i$ at line 9 represents a change in the initial id bit string of $i$ to each $i'$ which eventually will become $j$.\\

\begin{algorithm}[H]
For each processor $0 \leq i \leq N-1$, pick a random processor $\sigma(i)$ from $\{0, ... , N-1\}$ with a uniform distribution\;
Simultaneously Route each $v_i$ to $\sigma(i)$ using {\bf bitFixRoute}($i, v_i, \sigma(i)$)\;
Once all $v_i$ arrive at their $\sigma(i)$, simultaneously route each $v_i$ from $\sigma(i)$ to their $d(i)$ using
{\bf bitFixRoute}($\sigma(i), v_i, d(i)$)\;
\caption{{\bf Randomized Routing}}
\end{algorithm}\creturn



\begin{lem}
Let $e'$ and $e''$ be any two edges in the hypercube. Let $\mathcal{T}(e)$ be the the number of packet routing paths in the cube that use edge $e$. Then $\E[\mathcal{T}(e')] = \E[\mathcal{T}(e'')] = \frac{1}{2}$.
\end{lem}
\begin{proof}
For all $0 \leq i \leq N-1$, let $\rho_i$ be the routing path taken by $v_i$ from processor $i$ to processor $\sigma(i)$. Then for an edge $e$ in the hypercube,
\[
\E[\mathcal{T}(e)] = \sum_{i = 0}^{N-1} \E\Big[\indicator[e \in \rho_i]\Big].
\]
For a given path $\rho_i$, $\E\Big[\indicator[e \in \rho_i] \Big]$ is equal to $\Pr(e \in \rho_i)(1) + \Pr(e \notin \rho_i)(0) = \Pr(e \in \rho_i)$. Therefore
\begin{equation*}
 \E[\mathcal{T}(e)] = \sum_{i = 0}^{N-1} \Pr(e \in \rho_i).
 \end{equation*}

As far as our routing protocol is concerned, taking an edge in the hypercube is equivalent to switching a bit in a starting processor's $n$-bit binary id representation. Moving a packet along a communication link moves the packet to a processor with the same binary string id except with a change on the first, second, third, etc. bit in the id bit string. Therefore, each edge encodes a change on a particular index bit in the id bit string and encodes either a switch from 0 to 1 or a switch from 1 to 0.\\

Let $e = (v,w)$ be an arbitrary edge in the hypercube that goes from processor $v$ to processor $w$ that encodes an arbitrary switch on an arbitrary bit index $1 \leq \mu \leq n$. Because the bit fixing protocol progressively changes bits from right to left, we know a packet will only traverse the edge if the $\mu$-th though the $n$-th bit of the starting processor's ids are the same as that of $v$. Therefore, there are $2^{\mu-1}$ possible packets that can use $e$ as there are exactly $2^{\mu-1}$ processors that have identical $\mu$-th through $n$-th bits in their initial id bit string.\\

If we consider the bit fixing protocol again, the probability that any of these $2^{\mu-1}$ packets takes edge $e$ is dependent on the probability that the first $\mu-1$ bits in the destination processor match that of $v$, and the $\mu$-th bit of the destination processor does not match that of $v$. Because the random destination processor is chosen uniformly amongst all possible destination processors, the probability that any particular bit is a 0 in the destination processor is $\frac{1}{2}$, as is the probability that any particular bit is a 1. Because the bits that appear in the destination processor are independent of each other, the probability that a particular $k$ bits are in the destination processor is  $2^{-k}$. The probability that any particular one of these $2^{\mu-1}$ packets' paths uses $e$ is $2^{-\mu}$. Therefore, we know there are a total of $2^{\mu-1}$ packets that each have a probability of $2^{- \mu}$ of taking $e$, and some remaining $2^n - 2^{\mu-1}$ packets that have probability 0 of taking $e$. Thus, 
\[
\begin{aligned}
 \E[\mathcal{T}(e)] &= \sum_{i = 0}^{N-1} \Pr(e \in \rho_i)\\
 &= 2^{\mu - 1}(2^{-\mu}) + (2^{n} - 2^{\mu-1})(0)\\
 &= \frac{1}{2}.
 \end{aligned}
\]
\creturn
 \end{proof}
 
While routing the packets to their destinations, a packet is always transmitted down the the next edge in it's path unless it is waiting in the queue of a processor because another packet is being transmitted along said edge. Each time step that a packet spends waiting in a queue rather than being transmitted over an edge, until it reaches it's final destination, contributes to the delay of that packet. To this end, given an arbitrary packet $v_i$ with path $\rho_i = (e_1, e_2, ..., e_{\ell})$, if $v_i$ reaches it's destination at timestep $j$, then the total delay for that packet is $j - \ell$. We will show that this delay can be bounded by the the number of intersecting paths with $\rho_i$, but first we must prove the following.

\begin{lem}
Let $v_i$ and $v_{i'}$ be packets being routed from starting processors $i$ and $i'$ respectively to destination processors $h$ and $h'$ respectively in our hypercube such that $i \neq i'$. Let $\rho_i = (e_1, e_2, ..., e_{\ell})$ and $\rho_{i'} = (e'_{1}, e'_{2}, ..., e'_{\ell})$ be the paths of these packets as determined by the {\bf bitFixRoute} protocol. Suppose $\rho_i$ and $\rho_{i'}$ intersect. Let $(e_j \in \rho_i) = (e'_{j'} \in \rho_{i'})$ be the first edge shared by both $\rho_i$ and $\rho_{i'}$ such that $e_j$ is the $j$-th edge of $\rho_i$ and the $j'$-th edge of $\rho_{i'}$. The first point at which paths $\rho_i$ and $\rho_{i'}$ diverge after sharing an edge is the edge with the lowest $k$ such that $(e_{j +k} \in \rho_i) \neq (e'_{j' +k} \in \rho_{i'})$. At the edge at which paths $\rho_i$ and $\rho_{i'}$ diverge, the two paths will never re-converge and share no remaining edges.
\end{lem}

\begin{proof}
For any processor $i$, routing a packet $v_i$ can be treated conceptually the same as changing the bits from left to right of the initial id bit string of $i$ to those of the destination processor $g$'s id bit string. To this end, $v_i$ being in the queue of processor $h$ represents the intermediate state of the bit string when changing between $i$ and $g$. Furthermore, $v_i$ taking edge $e = (g_1, g_2)$ represents switching a bit in the id that moves the intermediate state of the id from $g_1$ to $g_2$.
\\

Let $e_{j +k} = (g_1, g_2)$ and $e'_{j' + k} = (g'_1, g'_2)$. Because edge $e_{j +k}$ is the first edge where the paths of $\rho_i$ and $\rho_{i'}$ diverge after previously  having converged, we know that both $g_1 = g'_1$.  Furthermore, since the paths diverge at $e_{j+k}$ we know $g_2 \neq g'_2$. Let $e_{j+k}$ encode  a change on the $\mu$-th bit and let $e'_{j' +k}$ encode a change on the $\mu'$-th bit. Since the paths diverge at these edges, we know $\mu \neq \mu'$. WLOG let $\mu <  \mu'$. Therefore, the first $\mu' -1$ bits of $g'_2$ must be the same as the first $\mu' -1$ bits of $g_2$, except for the $\mu$-th bit. The $\mu-th$ bit of $g'_2$ was not flipped, unlike the $\mu$-th bit of $g_2$ which was flipped from $g_1$. The only way the two paths converge again to take the same edge is if the state of the intermediate processor id's match again. Therefore, if the path of $v_{i'}$ meets back up with $v_i$ after it diverges, the $\mu$-th bit of the id of $i'$ must be flipped later to match that of $i$. However, the bit fixing protocol switches bits incrementally from lower index to higher index, so the $\mu$-th bit would be flipped before the $\mu'$-th bit by the protocol. Therefore, the only way for the path of $\rho_i$ and $\rho_{i'}$ to share edges  again would contradict the bit fixing protocol. So it must be that the paths of $v_i$ and $v_{i'}$  cannot re-converge. \\
\end{proof}
 
\begin{lem}
Let $v_i$ be a packet being routed from processor $i$ to processor $j$ via path $\rho_i$. Let $\mathcal{P}$ be the set of processors other than $i$ which have packets with routes that pass through at least one edge in path $\rho_i = (e_1, e_2, ..., e_{\ell})$. Let $D_i$ be the delay of packet $v_i$. Then, the delay $D_i$ is bounded such that $D_i \leq \lvert \mathcal{P} \rvert$.
\end{lem}

\begin{proof}
To prove that $D_i$ is bounded by $\lvert \mathcal{P} \rvert$, we will show that  there exists an injection from each delay of $v_i$ to a unique element in $\mathcal{P}$.\\

Let $v$ be a packet with starting processor in $\mathcal{P}$. We say $v$ has left $\rho_i$ just after the last time step at which it traverses any edge in $\rho_i$ ends. We define $L(v, t)$, the function for the lag of $v$ at time step $t$, as follows. Let $e_j$ be the last edge traversed or being traversed by $v$ in $\rho_i$ at time step $t$, where $j = 0$ if $v$ has yet to traverse an edge in $\rho_i$ at time step $t$. Define $L(v, t) = t-j$. Therefore, $D_i = L(v_i, t_f) = t_f - \ell$ where $t_f$ is the final time step before $v_i$ reaches it's destination.\\

At any time step $t$ where $L(v, t) -1 = L(v, t-1)$, we know there must be some other packet from $\mathcal{P}$ traversing $e_{t - L(v, t)+1}$ as otherwise $v$ would be traversing that edge and its lag would not have increased. Let this packet be $v'$. So, $L(v', t) = L(v, t-1)$, and there exists some packet $v'$ with lag $L(v, t) -1$. Therefore, at any particular time step $t_k$ where $L(v_i,t_k) -1 = L(v_i, t_k-1)$, we know there exists some packet at time step $t_k$ with lag $L(v_i,t_k) -1$.\\

 Let $t^*$ be the last time step during which which any packet using an edge in $\rho_i$ has lag $L(v_i,t_k) -1$. Let $v^*$ be a packet such that $L(v^*, t^*) = L(v_i, t_k)-1$.  Let $e_{j^*}$ be the last edge traversed or being traversed by $v^*$ at time step $t^*$ such that $j^* = t^* - L(v^*, t^*)$. We claim that $v^*$ traverses $e_{j^*}$ at time $t^*$.\\

Suppose for the sake of contradiction that $v^*$ does not traverse $e_{j^*}$ at time $t^*$. Then, $v^*$ must be waiting in a queue to take $e_{j^* + 1}$ as it has yet to leave $\rho_i$. So, at time step $t^* + 1$, $v^*$ either traverses edge $e_{j^* +1}$ or waits in the queue for another time step. If $v^*$ traverses $e_{j^* +1}$ at time $t^* + 1$, then
\[ L(v^*, t^* + 1) = (t^* + 1) - (j^* + 1) = t^* - j^* = L(v_i, t_k)-1,
\]
which contradicts $t^*$ as the last time step at which any packet in $\rho_i$ has lag $L(v_i,t_k) -1$. On the other hand, if $e_{j^* +1}$ waits in the queue for another time step, then
\[
L(v^*, t^* + 1) = (t^* + 1) - (j^*) = L(v^*, t^*) + 1 = L(v_i,t_k)
\]
which, by our earlier result, means that during time $t^* +1$, some packet in $\rho_i$ has lag $L(v_i,t_k) -1$, again contradicting $t^*$ as the last time step at which any packet in $\rho_i$ has lag $L(v_i,t_k)-1$. Thus, $v^*$ traverses $e_{j^*}$ at time $t^*$. We now claim that $v^*$ leaves $\rho_i$ at the finish of time step $t^*$. Suppose for the sake of contradiction that $v^*$ does not leave $\rho_i$ at the finish of time step $t^*$. Then, some packet traverses $e_{j^* + 1}$ at timestep $t^* + 1$, contradicting $t^*$ as the last time step at which any packet in $\rho_i$ has lag $L(v_i,t_k) -1$.\\

Therefore, for all $1 \leq q \leq L(v_i, t_f)$, a packet leaves $\rho_i$ with lag $q$. Since paths never re-converge after diverging, each of these packets must be unique. For each delay increase, there is a unique processor from $\mathcal{P}$ whose packet leaves $\rho_i$. Thus, there exists an injection from each delay of $v_i$ to an element in $\mathcal{P}$, and the total delay must be bounded by $\lvert \mathcal{P} \rvert$.\\
\end{proof}

\begin{thm}
Let $D_i$ be the delay incurred by the path $\rho_i$ when routing from $i$ to $\sigma(i)$. Then $\E[D_i] \leq \frac{3n}{4}$.
\end{thm}

\begin{proof}

%%% ============== I don't think this gets used in the proof at all ================= %%%
\iffalse
We define the delay $D_i$ incurred on a path $\rho_i$ to be the total amount of steps that the packet $v_i$ is not transmitted over a communication link, but rather waiting in a queue. We define $D$ as $\sum_{i=0}^{N-1}D_i$. So, 
\[
\E[D] = \sum_{i=1}^{N-1} \E[D_i]
\]
\fi
%%%  ================================================================ %%%

As established in lemma 1.3, the delay $D_i$ incurred for a packet originating from processor $i$ is at most $\lvert \mathcal{P} \rvert$, where $\mathcal{P}$ is the set of processors other than $i$ which have packets with routes that pass through at least one edge in $\rho_i$. Since $\lvert \mathcal{P} \rvert$ is the total number of paths that intersect path $\rho_i$, we can represent $\lvert \mathcal{P} \rvert$ as
\begin{equation}
\sum_{j \in \{0,...,n-1\}\setminus i} H_{ij},
\end{equation}
where $H_{ij}$ is the indicator variable that at least one edge in $\rho_j$ intersects with some edge in $\rho_i$. Thus, we can say for all $0 \leq i \leq N-1$
\begin{equation}
\sum_{j \in \{0,...,n\}\setminus i} H_{ij} \leq \sum_{k=1}^{\ell}\mathcal{T}(e_k),
\end{equation}
since each $\rho_j$ that intersects with $\rho_i$ must contribute at least 1 to some $\mathcal{T}(e_k)$.\\
\\
Now, to bound the expected delay in a path.
\begin{equation*}
\begin{aligned}
\E[D_i] &\leq \E\Big[\sum_{j \in \{0,...,n\}\setminus i} H_{ij}\Big]\\
&\leq \E\Big[\sum_{e \in \rho_i} \mathcal{T}(e)\Big].\\
\end{aligned}
\end{equation*}
Since $\rho_i$ is itself a random variable, we use the law of total expectation to show the following
\begin{equation}
\begin{aligned}
\E\Big[\sum_{e \in \rho_i} \mathcal{T}(e)\Big] &= \sum_{\rho \in P} \bigg(\Pr\Big(\rho_i = \rho\Big) \sum_{e \in \rho}\E\Big[ \mathcal{T}(e) \lvert \rho_i = \rho\Big]\bigg),
\end{aligned}
\end{equation}
where $P$ is the set of all possible paths originating from processor $i$.\\
\\
We know $e \in \rho$, $\E[\mathcal{T}(e) \lvert \rho_i = \rho]$ will not be the same as $\E[\mathcal{T}(e)]$. So we calculate the following. Fixing $e \in \rho$,

\begin{equation}
\begin{aligned}
\E\Big[\mathcal{T}(e) \Big\lvert \rho_i = \rho \Big] &= \E\Big[  \indicator[e \in \rho_i] + \sum_{j \in \{0, ..., n-1\} \setminus i}\indicator[e \in \rho_j] \Big\lvert \rho_i = \rho \Big]\\
&=  \E\Big[ \indicator[e \in \rho_i] \Big\lvert \rho_i = \rho \Big] + \E\Big[\sum_{j \in \{0, ..., n-1\} \setminus i}\indicator[e \in \rho_j] \Big\lvert  \rho_i = \rho \Big]\\
&= 1 + \sum_{j \in \{0, ..., n-1\} \setminus i}\E\Big[\indicator[e \in \rho_j] \Big\lvert  \rho_i = \rho \Big].\\
\end{aligned}
\end{equation}
Since the paths are chosen independently of each other, whether or not we have any edge $e \in \rho_j$ is independent of whether we have $\rho_i = \rho$. Therefore, 
\begin{equation}
\begin{aligned}
\E\Big[\mathcal{T}(e) \Big\lvert \rho_i = \rho \Big] &= 1 + \sum_{j \in \{0, ..., n-1\} \setminus i}\E\Big[\indicator[e \in \rho_j] \Big\lvert  \rho_i = \rho \Big]\\
&= 1 + \sum_{j \in \{0, ..., n-1\} \setminus i}\E\Big[\indicator[e \in \rho_j] \Big]\\
&\leq 1 + \sum_{j \in \{0, ..., n-1\}}\E\Big[\indicator[e \in \rho_j] \Big]\\
&= 1 + \E[\mathcal{T}(e)]\\
&= \frac{3}{2}.
\end{aligned}
\end{equation}

We can take our result from equation 1.9 to say
\begin{equation}
\begin{aligned}
\sum_{\rho \in P} \bigg(\Pr\Big(\rho_i = \rho\Big) \sum_{e \in \rho}\E\Big[ \mathcal{T}(e) \lvert \rho_i = \rho\Big]\bigg) &\leq \sum_{\rho \in P} \bigg(\Pr\Big(\rho_i = \rho\Big) \sum_{e \in \rho}\frac{3}{2}\bigg). \\
&= \sum_{\rho \in P} \bigg(\Pr\Big(\rho_i = \rho\Big)\lvert \rho \rvert\frac{3}{2}\bigg)\\
&= \sum_{\ell= 0}^n\bigg(Pr(\lvert \rho_i \rvert = \ell )\frac{3}{2}\ell\bigg).
\end{aligned}
\end{equation}

We know from the bit fixing protocol the length of the path is the same as the number of bits that need to be changed between the id representations of $i$ and $\sigma(i)$. Therefore, the number of paths of length $\ell$ is the number of combinations of $\ell$ bit changes that can be made. There are $\binom{n}{\ell}$ paths of length $\ell$, and a total of $2^n$ paths. So,
\begin{equation*}
\Pr(\lvert \rho_i \rvert = \ell) = \frac{\binom{n}{\ell}}{2^n},
\end{equation*}
and therefore,
\begin{equation}
\begin{aligned}
\E[D_i] &\leq \sum_{\ell= 0}^n\bigg(Pr(\lvert \rho_i \rvert = \ell )\frac{3}{2}\ell\bigg)\\
&= \sum_{\ell= 0}^n\bigg(\frac{\binom{n}{\ell}}{2^n}\frac{3}{2}\ell\bigg)\\
&=\frac{3}{2^{n+1}}\sum_{\ell=0}^n\binom{n}{\ell}\ell\\
&= \frac{3}{2^{n+1}}\cdot n2^{n-1}\\
&= \frac{3n}{2^2}\\
&=\frac{3n}{4}.
\end{aligned}
\end{equation}
\creturn
\end{proof}



%%%%%%% =============== THIS IS JUST WRONG =============== %%%%%%%
\iffalse
\begin{thm}
Let $\mathcal{S}$ be the total number of time steps required to route all packets from $i$ to $\sigma(i)$. Then $\E[\mathcal{S}] \leq 2^{n-2}5n$
\end{thm}

\begin{proof}

Let $\mathcal{S}_i$ be the total number of time steps required to route packet $v_i$ from $i$ to $\sigma(i)$. Then
\[
\E[\mathcal{S}] = \sum_{i = 0}^{n-1} \E[\mathcal{S}_i]
\]

Furthermore, the total number of steps required to route a packet is the length of the path plus the delay incurred on the path, so
\[
\E[\mathcal{S}_i] = \E[\lvert \rho_i \rvert ] + \E[D_i] \leq \frac{5n}{4}
\]
and therefore
\begin{equation}
\E[\mathcal{S}] \leq 2^n\frac{5n}{4} = 2^{n-2}5n
\end{equation}
\end{proof}
\fi
%%%%% ===================================================== %%%%%

\begin{thm}
Let $i$ be any processor with packet $v_i$. Let $D_i$ be the totaly delay incurred when routing $v_i$ from $i$ to intermediate destination processor $\sigma(i)$. Then $\Pr(D_i \geq 6n) \leq 2^{-7n}$.
\end{thm}

\begin{proof}
By theorem 1.4 the expected delay incurred when routing for all packets is bounded from above by $\frac{3n}{4}$. Since the $D_i$ themselves are bounded above by the sum of the independent 0-1 valued random variables $H_{ij}$, we can run a Chernoff bound on the individual $D_i$. So we get the probability that the delay of the packet going out of processor $i$, $D_i$, is greater than its expected value $\mu$ by a factor of $(1 + \delta)$ is
\begin{equation}
\begin{aligned}
\Pr(D_i \geq (1 + \delta)\mu) &\leq \left[ \frac{e^{\delta}}{(1 + \delta)^{(1 + \delta)}} \right]^{\mu}\\
&\leq \left[ \frac{e^{\delta}}{(1 + \delta)^{(1 + \delta)}} \right]^{\frac{3n}{4}}.
\end{aligned}
\end{equation}

We have $6n = 8\frac{3n}{4}$. Therefore, if we make $\delta = 7$, then 
\begin{equation}
\begin{aligned}
\Pr(D_i \geq (1 + 7)\mu) &\leq \Pr\Big(D_i \geq (8)\frac{3n}{4}\Big)\\
&\leq \left[ \frac{e^{7}}{8^{8}} \right]^{\frac{3n}{4}}\\
&= \left[ \frac{e^{\frac{21n}{4}}}{2^{18n}} \right]\\
&\leq  \left[ \frac{(2^2)^{\frac{21n}{4}}}{2^{18n}} \right]\\
&\leq 2^{-7n}.
\end{aligned}
\end{equation}
\end{proof}

\begin{thm}
The algorithm {\bf Randomized Routing} runs in less than $14n$ steps with probability greater than $1- 2^{-6n +1}$.
\end{thm}

\begin{proof}
First, we consider the probability that the first routing step, when routing all packets from their starting processor $i$ to their intermediate processor $\sigma(i)$, takes $7n$ or more time steps. Let $A_1$ be the event that the first routing step takes at least $7n$ time steps. By theorem 1.12, the probability that any processor $i$ experiences a delay of greater than $6n$ steps is less than $2^{-7n}$. Since the length of a packet's path is at most length $n$, the probability that any processor takes $7n$ or more time steps to route to it's intermediate destination must be less than $2^{-7n}$.

Now, in the event that the first routing step takes at least $7n$ steps, we know that at least one processor must have taken greater than $7n$ steps to complete their route. Let $A_1^j$ be the event that processor $j$ takes at least $7n$ steps to route to it's final destination. We can therefore consider the event $A_1$ that the first routing step takes at least $7n$ steps to be
\[
A_1 = \bigcup_{i = 0}^{N-1} A_1^i.
\]
So,
\[
\begin{aligned}
\Pr(A_1) &\leq \sum_{i = 0}^{N-1}\Pr(A_1^i)\\
&\leq \sum_{i=0}^{N-1}2^{-7n}\\
&= 2^n(2^{-7n})\\
&= 2^{-6n}.
\end{aligned}
\]
\creturn

Now, we consider the probability that the second routing step, when routing all the packets from their intermediate processor $\sigma(i)$ to their destination processor $d(i)$, takes $7n$ or more time steps. Let $A_2$ be the event that the second routing step takes at least $7n$ time steps. The second routing step is nearly the same as the first routing step, except reversed so that the random processor is the starting processor. We can determine by the same analysis of the first routing step that the probability of $A_2$ is less than or equal to $2^{-6n}$.\\

Let $A$ be the event that the {\bf Randomized Routing} algorithm runs in at least $14n$ time steps. Should event $A$ occur, we know that the first routing step has taken at least $7n$ steps or the second routing step has taken at least $7n$ steps. So, 
\[
A \subseteq A_1 \cup A_2,
\]
and therefore, 
\[
\begin{aligned}
\Pr(A) &\leq \Pr(A_1 \cup A_2)\\
&\leq \Pr(A_1) + \Pr(A_2)\\
&= 2^{-6n+1}.
\end{aligned}
\]

Thus, the probability that  {\bf Randomized Routing} runs in less than $14n$ steps is 
\[
\begin{aligned}
\Pr(\overline{A}) &= 1- \Pr(A)\\
&\geq 1 - 2^{-6n + 1}.
\end{aligned}
\]
\end{proof}

















\section{Minimum Cuts and Karger's Algorithm}
In this section, we present Karger's algorithm \cite{kargersalg} and its applications to minimum cut problems on undirected graphs. Despite the simplicity of Karger's algorithm, the algorithm can be used to both find and count global minimum cuts with high probability in polynomial time. As a consequence of Karger's algorithm, we can also easily place a bound on the number of minimum cuts that can emerge in an undirected graph. Here we present a generalized variant of Karger's. To this end, while it may not be clear from our generalized construction of the algorithm, there are implementations of Karger's algorithm that run in $O(\lvert E \rvert)$ time and space, where $E$ is the set of edges in the input graph. The presentation of our analysis derives from \cite{kargersalg} and \cite{princetonpaper} \\

Karger's algorithm is a randomized algorithm that attempts to find the minimum cut in a connected undirected graph $G= (V,E)$, with any number of duplicate edges between vertices. A minimum cut is defined as a partition of the vertex set $V$ into two disjoint vertex subsets $(S^*, \overline{S^*})$ such that $\overline{S^*} = V \setminus S^*$, both $S^* \neq \varnothing$ and $\overline{S^*} \neq \varnothing$, and the number of edges that have one vertex in $S^*$ and one vertex in $\overline{S^*}$ is minimal. First we define the subroutines, then Karger's algorithm is as follows. \\

\begin{algorithm}[H]
\Struct{superNode}{
    {\bf set}\ vertices\;
    \BlankLine\BlankLine
    \tcc{\textnormal{A {\it superNode} is a vertex in the graph that contains a set of vertices. During the initial state of the graph $G$, we consider each vertex to be a {\it superNode} with only itself in its vertex set.\\}}}
    
    
\BlankLine\BlankLine
\KwIn{Super nodes $u$ and $v$ in an undirected connected graph $G = (V,E)$ with no self-loops}
Let $w$ = {\bf new} {\it superNode}\;
$V = V \cup \{w\}$\; 
$w$.vertices = $u$.vertices $\cup$ $v$.vertices\;
\For{all edges $e = (u,v) =(v,u)$} {
	Remove $e$ from $E$\;
}
\For{all edges that $e = (u, u')$, where $u'$ is any vertex} {
$e' = (w, u')$\;
$E = E \cup \{e'\}$\;
Remove $e'$ from $E$\;
}
\For{all edges that $e = (v, v')$, where $v'$ is any vertex} {
$e' = (w, v')$\;
$E = E \cup \{e'\}$\;
Remove $e'$ from $E$\;
}
Remove $u$ and $v$ from $V$\;
\caption{Subroutine {\bf Collapse($G,u,v$)}}
\end{algorithm}
\begin{algorithm}[H]
\KwIn{Undirected connected graph $G = (V,E)$ with no self loops, where $E$ is the set of edges and $V$ is the set of vertices in $G$}
\KwOut{$S \subset V$ such that the partition ($S$, $\overline{S}$) defines a cut of $G$}
\While{$\lvert V \rvert > 2$} {
	Select a random edge $e = (u,v) \in E$\;
	\bf{Collapse}($u,v$)\;
}
Let the remaining two nodes be $v_1$ and $v_2$\;
Let $S = \{$the original vertices that comprise supernode $v_1$\}\;
{\bf Output}: $S$\;
\caption{Karger's Algorithm}
\end{algorithm}
\creturn\\

\begin{lem}
Let $G_i = (V_i, E_i)$ be the state of the graph $G = (V, E)$ after the $i^{th}$ iteration of the collapsing procedure loop in Karger's algorithm from lines 1 to 4. Let $S_i$ be any set of vertices such that $(S_i, \overline{S_i})$ is a partition that defines a cut on the graph of $G_i$, and let $C_i$ be the set of edges across the cut. Then, given the cut defined by $(S_i, \overline{S_i})$ on $G_i$, there exists a cut defined by some partition $(S, \overline{S})$ on $G$ with set $C$ of edges across the cut such that $(i) \ \bigcup_{v \in S_i} v.\text{vertices} = \bigcup_{v \in S} v.\text{vertices}$, $(ii) \ \bigcup_{v \in \overline{S_i}} v.\text{vertices} = \bigcup_{v \in \overline{S}} v.\text{vertices}$, and $(iii) \ \lvert C_i \rvert = \lvert C \rvert$.
\end{lem}

\begin{proof}
We will prove this claim by induction. We start with base case $i = 0$. The state of $G_0$ is the state of $G$ before any iteration of the collapsing loop. So, $G = G_0$. Trivially, any cut defined by the partition $(S_0 = S, \overline{S_0} = \overline{S})$ on $G_0$ is the same cut on $G$. Therefore,
\[
\bigcup_{v \in S_0} v.\text{vertices} = \bigcup_{v \in S} v.\text{vertices},
\]
\[
\bigcup_{v \in \overline{S_0}} v.\text{vertices} = \bigcup_{v \in \overline{S}} v.\text{vertices},
\]
and the size of the cut defined by the partition created by $(S_0, \overline{S_0})$ is the same as the size of the cut defined by the partition created by $(S, \overline{S})$, and our claim holds.\\

Now, for the inductive case, assume that our claim holds for all $i < k$, show that our claim holds when $i = k$. Let $S_k, \overline{S_k} $ define a cut on $G_k$. Now, from the end the $k - 1^{st}$ iteration to the end of the $k^{th}$ iteration of the collapsing procedure loop, we know some nodes $u_k$ and $v_k$ were collapsed into a single node $w_k$ such that $w_k.\text{vertices} = u_k.\text{vertices} \cup v_k.\text{vertices}$. We know $w_k$ is exclusively either in $S_k$ or $\overline{S_k}$, so WLOG assume $w_k \in S_k$ as otherwise we would just consider $\overline{S_k}$.\\

Let $S_{k-1} = (S_k \setminus \{w_k\}) \cup \{u_k, v_k\}$. None of the nodes other than $u_k,v_k,$ and $w_k$ were added or removed between the $k-1^{st}$ and $k^{th}$ iterations of the collapsing procedure loop, $\overline{S_{k-1}} = \overline{S_k}$. So trivially, 
\[
\bigcup_{v \in \overline{S_k}} v.\text{vertices} = \bigcup_{v \in \overline{S_{k-1}}} v.\text{vertices}.
\]
Furthermore, it also follows that
\[
\bigcup_{v \in S_k \setminus \{w_k\}} v.\text{vertices} = \bigcup_{v \in S_{k-1} \setminus \{u_k, v_k\}} v.\text{vertices},
\]
since none of those node's vertices sets were changed between the iterations. Therefore, because $w_k.\text{vertices} = u_k.\text{vertices} \cup v_k.\text{vertices}$ (line 6 of subroutine {\bf Collapse}), it so follows that
\[
\bigcup_{v \in S_k} v.\text{vertices} = \bigcup_{v \in S_{k-1}} v.\text{vertices}.
\]
\creturn
Let, $C_k \subseteq E_k$ be the set of edges across the cut defined by partition $(S_k, \overline{S_k})$. let $C_{k-1} \subseteq E_{k-1}$ be the set of edges across the cut defind by the partition $(S_{k-1}, \overline{S_{k-1}})$. We split $C_{k}$ into disjoint subsets $C_{k}^\varnothing$ and $C_k^{w_k}$, where $C_k^{w_k}$ is the set of edges in $C_k$ that have $w_k$ as an endpoint and $C_k^{
\varnothing}$ is the set of edges in $C_k$ that do not have $w_k$ as an endpoint. We also split $C_{k-1}$ into disjoint subsets $C_{k-1}^\varnothing, C_{k-1}^{u_k}, C_{k-1}^{v_k}, C_{k-1}^{u_k,v_k}$ where $C_{k-1}^{u_k}$ is the set of edges in $C_{k-1}$ with $u_k$ and not $v_k$ as an endpoint, $C_{k-1}^{v_k}$ is the set of edges in $C_{k-1}$ with $v_k$ and not $u_k$ as an endpoint,  $C_{k-1}^{u_k,v_k}$ is the set of edges with both $u_k$ and $v_k$ as endpoints, and $C_{k-1}^\varnothing$ is the set of edges in $C_{k-1}$ that have neither $u_k$ nor $v_k$ as endpoints.
\\

Because the only edges and nodes that were changed or affected in any manner between iterations $k-1$ and $k$ of the collapsing procedure loop were nodes $u_k$, $v_k$, or $w_k$ and any edges that had these as an endpoint, $\lvert C_{k-1}^\varnothing \rvert = \lvert C_k^\varnothing$. Since $u_k$ and $v_k$ are on the same side of the partition in $S_{k-1}$, $C_{k-1}^{u_k,v_k} = \varnothing$, so $\lvert C_{k-1}^{u_k,v_k}  \rvert = 0$. Let $e' = (a', b') \in (C_{k-1}^{u_k} \cup C_{k-1}^{v_k}$ be any edge where $a' = u_k \tor a' = v_k$ and $b'$ is any vertex. Since $e'$ is an edge across the cut, we know $b' \in \overline{S_{k-1}}$ and therefore $b' \in \overline{S_{k}}$ . Furthermore, since $a' = u_k \tor a' = v_k$, we know during the $k^{th}$ iteration of the collapsing procedure loop we will remove edge $e'$ from the graph and add a new edge $e'' = (w_k, b')$ to the graph. Because $w_k \in S_{k}$, $e''$ is an edge across the cut, so $e'' \in C_{k}^{w_k}$. So, for each edge in $e' \in (C_{k-1}^{u_k} \cup C_{k-1}^{v_k})$ in the graph of $G_{k-1}$, we remove e' and add a new edge $e'' \in C_{k}^{w_k}$, so $\lvert (C_{k-1}^{u_k} \cup C_{k-1}^{v_k}) \rvert \leq \lvert C_{k}^{w_k} \rvert$.\\

Now, consider any edge $e'' = (w_k, b'') \in C_k^{w_k}$. Since $e''$ has $w_k$ as an endpoint, then during the $k^th$ iteration, it was added to the graph only after some edge $e' = (u_k, b'') \tor e' = (v_k, b'')$ was removed from the graph. Since $e'' \in C_k^{w_k}$, then $b'' \in \overline{S_k}$, which means also $b'' \in \overline{S_{k-1}}$. As we already know, both $u_k \in S_{k-1}$ and $v_k \in S_{k-1}$. Therefore,  $e' \in (C_{k-1}^{u_k} \cup C_{k-1}^{v_k})$. Thus, for ever edge $e''$ added to the graph $C_k^{w_k}$, there was some edge removed from the graph in $C_{k-1}^{u_k} \cup C_{k-1}^{v_k}$, so $\lvert C_k^{w_k} \leq (C_{k-1}^{u_k} \cup C_{k-1}^{v_k})$.\\

Thus, $\lvert C_{k-1} \rvert = \lvert C_k \rvert$. Because the cut $(S_k, \overline{S_k}$ on $G_k$ with set of edges $C_k$ across the cut maps to a cut defined by partition $(S_{k-1}, \overline{S_{k-1}})$ on $G_{k-1}$ with set $C_{k-1}$ of edges across the cut such that $\bigcup_{v \in S_k} v.\text{vertices} = \bigcup_{v \in S_{k-1}} v.\text{vertices}$, $\bigcup_{v \in \overline{S_k}} v.\text{vertices} = \bigcup_{v \in \overline{S_{k-1}}} v.\text{vertices}$ and $\lvert C_k \rvert = \lvert C_{k-1} \rvert$, by the inductive hypothesis there must exist some vertex set $S$ such that the cut $(S_k, \overline{S_k}$ on $G_k$ maps to some cut defined by partition $(S, \overline{S})$ on $G$ with set $C$ of edges across that cut where $\bigcup_{v \in S_k} v.\text{vertices} = \bigcup_{v \in S} v.\text{vertices}$, $\bigcup_{v \in \overline{S_k}} v.\text{vertices} = \bigcup_{v \in \overline{S}} v.\text{vertices}$ and $\lvert C_k \rvert = \lvert C \rvert$.\\
\end{proof}


Due to the collapsing procedure defined by algorithm {\bf Collapse}, not all graph cuts in the graph of $G$ can necessarily be returned by Karger's algorithm. So we prove the following.

\begin{lem} Let $(S, \overline{S})$ be any vertex partition that defines a cut on an undirected connected graph $G = (V, E)$, where $C$ is the set of edges across the cut such that $C = \{ \forall e = (u,v) \in E \  \lvert \ u \in S, v \in \overline{S} \tor v \in S, u \in \overline{S} \}$. Let $G'$ be the graph after we collapse all edges between vertices in $S$ and edges between vertices in $\overline{S}$, such that the only edges that are not collapsed and remain in $G'$ are those in $C$. The cut defined by the partition $(S, \overline{S})$ is returned by Karger's when run on $G$ with probability $p > 0$ iff $G'$ consists of exactly two super nodes. 
\end{lem}
\begin{proof}
First, suppose $G'$ collapses to exactly two super nodes. Let $E_S$ be a set of edges such that, for each unordered pair of vertices $(u,v)$ in $S$ that share an edge, $E_S$ contains one edge $e=(u,v)$. Let $E_{\overline{S}}$ be a set of edges such that, for each unordered pair of vertices $(u,v)$ in $\overline{S}$ that share an edge, $E_{\overline{S}}$ contains one edge $e=(u,v)$. We can consider an instance of Karger's algorithm that collapses first the edges in $E_S$ and then the edges in $E_{\overline{S}}$. Since each edge is selected randomly and uniformly, we know there is a non-zero probability that this choice of edges to collapse occurs in that order. Furthermore, the collapsing procedure will stop after these edges are collapsed, and Karger's will return the set of vertices contained in one of the supernodes, which we know to be $S$ or $\overline{S}$. Therefore, running Karger's on $G'$ returns the cut defined by $(S, \overline{S})$ with a non-zero probability.\\

Now, assume $(S,\overline{S})$ does not reduce to 2 super nodes, then either the collapse of $S$ or $\overline{S}$ is not a single super node. Since Karger's algorithm only returns cuts consisting of two super-nodes, it cannot return $(S,\overline{S})$.\\
\end{proof}

\begin{lem}
Let $(S, \overline{S})$ be the vertex partition that defines a minimum cut on a connected undirected graph $G$. The cut defined by partition $(S, \overline{S})$ is returned by Karger's algorithm when run on $G$ with probability $p > 0$.
\end{lem}
\begin{proof}
Let $C$ be the set of edges across the cut such that $C = \{ \forall e = (u,v) \in E \  \lvert \ u \in S, v \in \overline{S} \tor v \in S, u \in \overline{S} \}$. Let $G'$ be the graph after we collapse all edges between vertices in $S$ and edges between vertices in $\overline{S}$, such that the only edges that are not collapsed and remain in $G'$ are those in $C$. We know by lemma 2.2, the cut defined by $S$ is returned with probability $p > 0$ iff $G'$ consists of exactly two super nodes.\\

Suppose for the sake of contradiction that $G'$ does not consist of exactly two super nodes. Therefore, either the collapse of $S$ or $\overline{S}$ is not a single super node. WLOG assume it's $S$. Let $A$ be the vertices contained by one of these super nodes of $S$. Because $G'$ is the collapsed graph of the partition of $S$, there are no remaining edges between the super nodes of $S$. Furthermore because $G$ is connected, we know that, since the super nodes of $S$ are not connected to each other, the super nodes of $S$ must be connected by edges to the super nodes of $\overline{S}$. Now, we consider cut defined by the vertex partition between $S \setminus A$ and $\overline{S} \cup A$. The weight of this cut must be the weight of the cut defined by $S$, subtracted by the weight of the edges between the super node containing $A$ and the other super nodes in $\overline{S}$, which is lower than the weight of the cut defined by $(S, \overline{S})$. However, this contradicts the minimality of the cut defined by $(S, \overline{S})$. Thus, our supposition must be incorrect, and $G'$ consists of exactly two super nodes, which means cut $S$ is returned by Karger's with probability $p > 0$.
\\
\end{proof}

\begin{lem}
Given any unweighted undirected graph $G = (V,E)$, let $v$ be a random vertex chosen uniformly from $V$. Let $\deg(v)$ be the degree of vertex $v$. Then $\E[\deg(v)] = \frac{2 \lvert E \rvert }{\rvert V \lvert}$.
\end{lem}
\begin{proof}
\begin{equation}\begin{aligned}
\E[\deg(v)] &= \sum_{u \in V} \Pr(v = u)\deg(u)\\
&= \sum_{u \in V} \frac{1}{\lvert V \rvert}\deg(u)\\
&= \frac{1}{\lvert V \rvert} \sum_{u \in V} \deg(u)\\
&= \frac{2 \lvert E \rvert}{\lvert V \rvert}
\end{aligned}
\end{equation}
\creturn
\end{proof}

\begin{lem}
Let $S^* \subset V$ be a vertex set such that the partition $(S^*, \overline{S^*})$ defines a minimum cut of graph $G = (V, E)$, and let $C^*$ define the set of edges across the cut with one vertex in $S^*$ and one vertex in $\overline{S^*}$. Then, $\lvert C^\ast \rvert \leq \frac{2 \lvert E \rvert }{\rvert V \lvert}$.
\end{lem}
\begin{proof}
Let $v$ be any vertex in $V$. Now, consider the edges across cut $C_v$ defined by the vertex partition of $\{v\}$ and $\overline{\{v\}} = V \setminus \{v\}$. We know the size of this cut $\lvert C_v \rvert = \deg(v)$ since all of the edges that cross the partition must be the edges which have $v$ as one endpoint.\\

Suppose that $\deg(v) \leq \frac{2\lvert E \rvert}{\lvert V \rvert}$. Since there exists a cut $C_v$ of size at most $\frac{2\lvert E \rvert}{\lvert V \rvert}$, then the the size of the minimum cut $\lvert C^* \rvert$ must be no greater than $\frac{2\lvert E \rvert}{\lvert V \rvert}$.\\

Now, suppose on the otherhand that  $\deg(v) > \frac{2\lvert E \rvert}{\lvert V \rvert}$. By Lemma 2.4 we know that the average vertex degree is $\frac{2 \lvert E \rvert}{\lvert V \rvert}$. Because $\deg(v) > \frac{2\lvert E \rvert}{\lvert V \rvert}$, we know there must exist some other vertex $v'$ that witnesses this average such that $\deg(v') < \frac{2\lvert E \rvert}{\lvert V \rvert}$. Now, we consider the vertex cut $C_{v'}$ between $\{v'\}$ and $\overline{\{v'\}} = V \setminus \{v'\}$. As already established, the size of this cut will be $\deg(v')$ which we know to be less than $\frac{2\lvert E \rvert}{\lvert V \rvert}$. It so follows that since there exists a cut of size less than $\frac{2\lvert E \rvert}{\lvert V \rvert}$, then the size of the minimum cut $\lvert C^* \rvert \leq \lvert C_{v'} \rvert < \frac{2\lvert E \rvert}{\lvert V \rvert}$.\\
\end{proof}

\begin{thm} Given a connected graph  $G = (V, E)$ with no self loops, let $C^*$ be the set of edges across any minimum cut of $G$ defined by partition $(S^*, \overline{S^*})$. Let $p^*$ be the probability that Karger's algorithm successfully outputs $S^*$. Then $p^* \geq \frac{2}{n^2}$, where $n = \lvert V \rvert$.
\end{thm}
\begin{proof}
Given the simple manner by which Karger's algorithm chooses which vertex partition to return, it's clear that Karger's algorithm returns a particular minimum cut if and only if none of the the edges in the cut are randomly chosen for contraction at step 3 in the algorithm. So, 
\begin{equation*}
\begin{aligned}
 p^*= \Pr(\text{no edge across cut }C^*\text{ is chosen by the algorithm}).\\
\end{aligned}
\end{equation*}
Since there are $n$ vertices, we know the algorithm contracts exactly $n-2$ edges before reaching 2 super nodes. Thus,
\begin{equation*}
\begin{aligned}
\Pr(\text{no edge}& \text{ across cut }C^*\text{ is chosen by the algorithm})\\
= \prod_{i=1}^{n-2} \Pr(&\text{no edge across } C^* \text{ is chosen  during the }i^{th} \text{ edge contraction } \lvert\\
&\text{no other edges had been chosen from the cut})\\
=\prod_{i=1}^{n-2} \bigg( 1& - \Pr(\text{an edge across } C^* \text{ is chosen  during the }i^{th} \text{ edge contraction} \lvert\\
&\text{no other edges had been chosen from the cut}) \bigg).\\
\end{aligned}
\end{equation*}

If $C^*$ is a minimum cut in $G$ in it's initial state, it must also be the minimum cut in $G$ in its state after any subsequent iterations of the vertex collapsing procedure loop. For the sake of contradiction, suppose that after the $j^{th}$ iteration of the procedure loop, there was another cut $C'$ on the current state of the graph such that $\lvert C^! \rvert < \lvert C^* \rvert$. By lemma 2.1, there would also need to be some cut on the input graph of $G$ with size $\lvert C^! \rvert$, thereby contradicting the minimality of $C^*$. Thus, $C^*$ must be a minimum cut in $G$ in its state after any subsequent iterations of the vertex collapsing procedure loop.\\

By lemma 2.6, we know that in any graph $G' = (V', E')$ with minimum cut defined by partition $(S', \overline{S'})$ with edges across the cut $C'$, the size of the minimum cut $\lvert C' \rvert \leq \frac{2\lvert E'\rvert}{\lvert V' \rvert}$. So, by simple algebra $\lvert E \rvert \geq \frac{\lvert C' \rvert \lvert V' \rvert}{2}$. Furthermore, we know that each time we perform the collapsing procedure, we collapse two nodes into one, thereby reducing the total number of the nodes in the graph by 1. Therefore, during the $i^{th}$ iteration of the collapsing procedure loop, there are $n-i + 1$ nodes in the graph of $G$ at the time. Let $G_j = (E_j, V_j)$ be the state of $G$ during the $j^{th}$ iteration of the collapsing procedure loop. We can therefore say that during the $i^{th}$ iteration of the loop when we're choosing which edge we will collapse, the size of the edge set $E_i$ is
\begin{equation*}
\begin{aligned}
\lvert E_i \rvert &\geq \frac{\lvert C^* \rvert \lvert V_i \rvert}{2}\\
&= \frac{\lvert C^* \rvert (n-i+1)}{2},\\
\end{aligned}
\end{equation*}
and therefore
\begin{equation}
\begin{aligned}
\Pr(\text{an edge across } C^* \text{ is chosen  during the }i^{th} \text{ edge contraction} \lvert\\
\text{no other edges had been chosen from the cut}) &=  \frac{\lvert C^* \rvert}{\lvert E_i \rvert}\\
&\leq \frac{\lvert C^* \rvert}{\lvert C^* \rvert(n-i + 1)/2}\\
&= \frac{2}{n-i + 1}.
\end{aligned}
\end{equation}
So, 
\begin{equation}
\begin{aligned}
p^* &\geq \prod_{i=1}^{n-2} 1 - \Pr(\text{an edge } C^* \text{ is chosen  during the }i^{th} \text{ edge contraction } \lvert\\
&\quad\quad\quad\quad\quad\quad \ \ \text{no other edges had been chosen from the cut})\\
&\geq \prod_{i = 1}^{n-2} 1- \frac{2}{n-i+1}\\
&= \left(1-\frac{2}{n}\right)\left(1-\frac{2}{n-1}\right)\left(1 - \frac{2}{n-2}\right)...\left(1-\frac{2}{4}\right)\left(1-\frac{2}{3}\right)\\
&= \left(\frac{n-2}{n}\right)\left(\frac{n-3}{n-1}\right)\left(\frac{n-4}{n-2}\right)...\left(\frac{2}{4}\right)\left(\frac{1}{3}\right)\\
&= \frac{2}{n(n-1)} = \frac{1}{\binom{n}{2}}\\
&\geq \frac{2}{n^2}.
\end{aligned}
\end{equation}
\end{proof}


\begin{cor}
Let $p$ be the probability that the vertex partition $(S', \overline{S'})$ found by Karger's algorithm is a minimum cut. Then $p \geq \frac{2}{n^2}$. 
\end{cor}
\begin{proof}
This is largely a consequence of theorem 2.7. Karger's algorithm can find a particular minimum cut with probability $p^* \geq \frac{2}{n^2}$. Since there can be more than one minimum cut in a graph, it follows that $p \geq p^* \geq \frac{2}{n^2}$.
\\
\end{proof}



\begin{thm}
Let $S$ be a set of vertices such that the partition $(S, \overline{S})$ defines a minimum cut on an unweighted undirected graph $G$.  For any $\ 0 < \delta \leq 1$, running $k$ iterations of Karger's algorithm on $G = (V,E)$ will return $S$ at some point with probability $p > 1 - \delta$, where $k \geq \frac{-n^2}{2}\log(\delta)$ and $n = \lvert V \rvert$. 
\end{thm}
\begin{proof}
Let $X_i$ be a random variable that indicates whether $S$ is returned by Karger's algorithm on the $i^{th}$ iteration of Karger's algorithm. Let $X = \sum_{i = 1}^{k} X_i$. Then $S$ is returned by one of the $k$ iterations of Karger's iff $X \geq 1$. Therefore, the probability that $S$ is returned by Karger's is
\[
\Pr(X \geq 1)  = 1 - \Pr(X  = 0) = 1 - \Pr\left(\bigcap_{i=1}^k X_i = 0 \right).
\]
Because each iteration of Karger's is independent of the others and all have identical probability for finding the cut,
\[
\Pr\left(\bigcap_{i=1}^k X_i = 0 \right) = \prod_{i  = 0}^k \Pr(X_i = 0) = \Big(\Pr(X_j = 0)\Big)^k,
\]
for any $1 \leq j \leq k$. We know by theorem 2.7, that the probability $p^*$ that a particular minimum cut is returned by Karger's is $p^* \geq \frac{2}{n^2}$. So, the probability $p'$ that $S$ is not returned by Karger's alogrithm is $p' \leq 1- \frac{2}{n^2}$. Therefore,
\[
\Big(\Pr(X_j = 0)\Big)^k \leq \Big(1- \frac{2}{n^2}\Big)^k,
\]
which we can rewrite as
\[
\Bigg( \bigg( 1 - \frac{1}{\frac{1}{2}n^2}\bigg)^{\frac{1}{2}n^2}\Bigg)^\frac{k}{\frac{1}{2}n^2}.
\]

For all  $m > 1$,  we have $0 < (1 - \frac{1}{m})^m \leq \frac{1}{e}$. So, for all graphs with more than 2 vertices,
\begin{equation}\begin{aligned}
\Big(\Pr(X_j = 0)\Big)^k &\leq \Bigg( \bigg( 1 - \frac{1}{\frac{1}{2}n^2}\bigg)^{\frac{1}{2}n^2}\Bigg)^\frac{k}{\frac{1}{2}n^2}\\
& \leq \Big( \frac{1}{e}\Big)^{\frac{2k}{n^2}}\\
&= \exp\bigg(\frac{-2k}{n^2}\bigg).
\end{aligned}
\end{equation}

Now, substituting $k \geq \frac{-n^2}{2}\log(\delta)$, we have
\[\begin{aligned}
\Big(\Pr(X_j = 0)\Big)^k &\leq  \exp\bigg(\frac{-2k}{n^2}\bigg)\\
&\leq \exp\Big(\log(\delta)\Big)\\
& = \delta.
\end{aligned}
\]

Thus, when $k \geq \frac{-n^2}{2}\log(\delta)$, for any $0 < \delta \leq 1$,
\[
\Pr(X \geq 1)  = 1 - \Big(\Pr(X_j = 0)\Big)^k \geq 1 - \delta.
\]
\\
\end{proof}


\begin{cor}
A particular minimum cut $(S, \overline{S})$ on $G$ is found with high probability using $O(n^2)$ iterations of Karger's algorithm.
\end{cor}
\begin{proof}
By theorem 2.11, we know that for any $0 < \delta \leq 1$, a particular minimum cut of $G$ is found in at least $\frac{-n^2}{2}\log(\delta)$ iterations of Karger's algorithm with probability $p > 1 - \delta$, which is
\[
O\left(\frac{-n^2}{2}\log(\delta)\right) = O\Bigg(n^2\log\bigg(\frac{1}{\delta}\bigg)\Bigg) = O\Big(n^2\Big)
\]
iterations of Karger's algorithm. 
\\
\end{proof}




\begin{thm}
There are no more than $\binom{n}{2}$ minimum cuts in any undirected unweighted graph $G = (V,E)$ where $\lvert V \rvert = n$
\end{thm}
\begin{proof}
Let $\mathscr{C}$ be the set of all possible cuts that can be found by partition returned from Karger's algorithm. Consider the the cut $C$ resulting from the vertex partition returned by Karger's algorithm. The events $(\textscr{c} = C)$ for all $\textscr{c} \in \mathscr{C}$ form a partition of the probability space. Therefore,
\[
\sum_{\textscr{c} \in \mathscr{C}} \Pr(\textscr{c} = C) = 1.
\]

By theorem 2.7, we know 
\[
\Pr(C^* = C) \geq \frac{1}{\binom{n}{2}},
\]
 for any minimum cut $C^*$. Let $\mathscr{C}^*$ be the set of all minimum cuts in $G$. So,
 \begin{equation}
 \begin{aligned}
1 &= \sum_{\textscr{c} \in \mathscr{C}} \Pr(\textscr{c} = C)\\
&\geq \sum_{\textscr{c}^* \in \mathscr{C}^*} \Pr(\textscr{c}^* = C)\\
&\geq \lvert \mathscr{C}^* \rvert \frac{1}{\binom{n}{2}}.
\end{aligned}
 \end{equation}
 Therefore, $\lvert \mathscr{C}^* \rvert \leq\binom{n}{2}$.
 \\
\end{proof}
The bound $\binom{n}{2}$ is also tight. Take for example the graph of the $n$-vertex cycle. We know the size of the minimum cut is $2$. Taking any two distinct edges in the cycle, there exists a valid cut on the cycle in which these two edges are the only two edges across the cut. Therefore, we can create a total of $\binom{n}{2}$ minimum cuts in the graph of the $n$-vertex cycle.



\begin{thm}
Let $\mathscr{S}$ be the set of all vertex partitions on an unweighted undirected graph $G$ such that $(i)$ for all $(S, \overline{S}) = (\overline{S}, S) \in \mathscr{S}$, $(ii)$ the partition $(S, \overline{S})$ defines a minimum cut. For any $0 < \delta \leq 1$, running $k$ iterations of Karger's algorithm on $G = (V,E)$ will return all $S$ such that $(S, \overline{S}) \in \mathscr{S}$ during the $k$ iterations with probability $p > 1 - \delta$, where $k \geq \frac{-n^2}{2}\log\left(\frac{\delta}{\binom{n}{2}}\right)$ and $n = \lvert V \rvert$. 
\end{thm}
\begin{proof}
Assume there are $m$ minimum cuts on the graph of $G$. Let $X$ be random variable indicating the event that the vertex sets that define all $m$ cuts of $G$ are returned during the $k$ iterations of Karger's. Assign an arbitrary unique id from 1 to $m$ to each minimum cut. Let $X_i$ be the random variable indicating the event that the the vertex set that defines the $i^{th}$ cut of $G$ is returned during the $k$ iterations of Karger's. So,
\[
\Pr(X = 1) = \Pr\left(\bigcap_{i = 0}^m X_i = 1\right) = 1 - \Pr\left(\overline{\bigcap_{i = 0}^m X_i = 1}\right) = 1 - \Pr\left(\bigcup_{i = 0}^m X_i = 0 \right),
\]
and therefore,
\[
\Pr\left(\bigcup_{i = 0}^m X_i = 0 \right) \leq \sum_{i = 0}^m \Pr(X_i = 0).
\]
By equation 2.12, we know for any graph $G$ with at least 2 vertices, the probability that a particular minimum cut is not found over $k$ iterations of Karger's algorithm is less than $e^{\frac{-2k}{n^2}}$. So,
\[
\begin{aligned}
\Pr\left(\bigcup_{i = 0}^m X_i = 0 \right) &\leq \sum_{i = 0}^m \Pr(X_i = 0)\\
&= m\exp\Big(\frac{-2k}{n^2}\Big).
\end{aligned}
\]
As proven in theorem 2.14, there are no more than $\binom{n}{2}$ minimum cuts in $G$. Thus,
\begin{equation}
\begin{aligned}
\Pr\left(\bigcup_{i = 0}^m X_i = 0 \right) &\leq m\exp\Big(\frac{-2k}{n^2}\Big)\\
&\leq \binom{n}{2}\exp\Big(\frac{-2k}{n^2}\Big).
\end{aligned}
\end{equation}

Now, substituting $k \geq \frac{-n^2}{2}\log\left(\frac{\delta}{\binom{n}{2}}\right)$,
\begin{equation}
\begin{aligned}
\Pr\left(\bigcup_{i = 0}^m X_i = 0 \right) &\leq \binom{n}{2}\exp\Big(\frac{-2k}{n^2}\Big)\\
&\leq \binom{n}{2}\exp\left(\log\bigg(\frac{\delta}{\binom{n}{2}}\bigg)\right)\\
 &= \delta.
\end{aligned}
\end{equation}
Thus, when $k \geq \frac{-n^2}{2}\log\left(\frac{\delta}{\binom{n}{2}}\right))$, for any $0 < \delta \leq 1$,
\[
\Pr(X = 1)  =  1 - \Pr\left(\bigcup_{i = 0}^m X_i = 0 \right) \geq 1 - \delta.
\]
\end{proof}

\begin{cor}
Every minimum cut in $G$ is found with high probability using $O(n^2 \log n)$ iterations of Karger's algorithm.
\end{cor}
\begin{proof}
By theorem 2.16, we know that for any $0 < \delta \leq 1$, every minimum cut in $G$ is found in at least $\frac{-n^2}{2}\log\left(\frac{\delta}{\binom{n}{2}}\right)$ iterations of Karger's algorithm with probability $p > 1 - \delta$, which is
\[
O\left(\frac{-n^2}{2}\log\left(\frac{\delta}{\binom{n}{2}}\right)\right) = O\left(n^2\log\left(\frac{\binom{n}{2}}{\delta}\right)\right) = O\Big(n^2 \log n\Big)
\]
iterations of Karger's algorithm.
\\
\end{proof}






























\begin{section}{Random Walks and Graph Connectivity}
\end{section}
In this section, we present a very simple algorithm to determine graph connectivity on an undirected graph. The algorithm takes the rather brute force approach of simply taking random edges until the destination vertex is reached, if ever. However, the analysis reveals that despite the simplicity of the method, the algorithm runs in polynomial time with high probability. Although our analysis primarily focuses on demonstrating the probabilistic run time of the algorithm, there exists a simple construction of our algorithm that runs in log-space. However, the details of such a machine construction are not pertinent to the paper and left as an exercise to the reader. The algorithm and analysis thereof adapts largely from sections 16.1 and 16.2 of \cite{unreleasedtextbook}.\\

We take the following notion of a random walk. Let $G = (V,E)$ be an undirected graph. Let $\Gamma(u)$ denote the set neighboring vertices of $u$ in $G$. We define a neighboring vertex as any vertex $v$ such that there exists an edge $e = (u,v)$. Now, we define a $T$-step random walk starting at $v$ to be a sequence of random variables $X_0, ..., X_T$ such that for all $0 \leq i \leq T$, $(i)$ $X_i \in V$, $(ii)$ $X_{i+1} \in \Gamma(X_i)$ and $X_{i+1}$ is randomly chosen uniformly from $\Gamma(X_i)$, and $(iii)$ $X_0 = v$. A $T$-step random walk is a path that starts at vertex $v$ where each edge taken in the path is chosen at random.\\

Now, let us introduce the {\it undirected path problem}. Given an undirected graph $G$, a starting vertex $s \in V$, and an ending vertex $t \in V$, the {\it undirected path problem} answers whether or not there exists a path in $G$ connecting $s$ to $t$. We will show that this problem can be solved in probabilistic polynomial time. However, before that, we must introduce the following graph reduction algorithm.\\

The intent of algorithm {\bf selfLooping4Regular} is to create a 4 regular graph wherein each vertex has at least one self-looping edge. A 4-regular graph is a graph in which every vertex has a degree of 4. We consider a self-loop to contribute an added degree of one to a vertex. We impose a labeling on our vertices from the numbers 1 to $n$, where $\lvert V \rvert = n$.\\

\begin{algorithm}[H]
\KwIn{An undirected graph $G$}
\KwOut{Undirected graph $G'$ such that $G'$ is a 4-Regular graph such that each vertex $v' \in G'$ has a self-loop}
Let $G' = (V',E')$ initially be the empty graph\;
\For{each vertex $v_u \in G$} {
add a cycle $c_u$ of size $n$ to $G'$\;
}
\For{each unique edge $e = (v_i, v_j) = (v_j, v_i) \in G$ such that $v \neq v$} {
add edge $e' = (c_i^j, c_j^i)$ to $G'$\;
\textsf{//Define $c_a^b$ to be the $b^{th}$ vertex of cycle $a^{th}$ cycle $c_a$ in $G'$}
}
\For{all $v' \in G'$} {
	\While{$\deg(v') < 3$} {
		add a self looping edge $e'' = (v', v')$ to $G'$\;
	}
	add a self looping edge $e'' = (v', v')$ to $G'$\;
}
{\bf Output}: $G'$\;

\caption{{\bf selfLooping4Regular}($G$)}
\end{algorithm}
\creturn
\begin{lem} $G'$ is a 4-Regular graph such that each vertex $v' \in G'$ has a self-loop.
 \end{lem}
\begin{proof}
Because the algorithm in the for loop at line 13 clearly adds a self-loop to each vertex $v' \in V'$, the claim that each vertex has a self loop is easily and trivially proven.\\

Now, we only need to prove the claim that each vertex $v' \in G'$ has degree $4$. Suppose for the sake of contradiction that there exists some vertex $v''$ such that $\deg(v'') \neq 4$. Clearly, $\deg(v'') > 4$ as during the while loop at line 10, we ensure the degree of every vertex is at least 3 before adding the last self loop to the vertex. Therefore, if $\deg(v'') \neq 4$, then $\deg(v'')
 > 4$. So, prior to the for loop at lines 9 through 14, $\deg(v'') > 3$. Suppose $v'' = c_{j'}^{i'}$. Because each vertex in $G'$ is in a cycle of $n$ vertices, we know two of the edges that go out of the vertex are the edges that connect $v''$ to it's cycle, so we know $v''$ as an edge connecting it to $c_{j'}^{i'-1 \mod n}$ and an edge connecting it to $c_{j'}^{i'+1 \mod n}$. Since $\deg(v'') >3$ before the for loop at line 9, we know there must be at least 2 more edges connected to $v''$ that are not the edges that connect $v''$ to it's cycle and were added in the for loop at line 9. Let $e'' = (v'', c_{j''}^{i''})$, and $e^! = (v'', c_{j^!}^{i^!})$ be any two of those edges.\\
 \\
For any $i, j, k, \ell$, the algorithm only adds an edge between the vertices $c_{j}^i$ and $c_{\ell}^k$ if $j \neq \ell$. Also, the algorithm adds an edge between two vertices $c_{j}^i$ and $c_{\ell}^k$ iff there exists an edge between $v_{j}$ and $v_{\ell}$ in $G$. Furthermore, considering the manner in which the algorithm adds edges to $G'$, if an edge between $v_{j}$ and $v_{\ell}$ exists in the graph, then the algorithm adds the edge between the $\ell^{th}$ vertex in the $j^{th}$ cycle and the $j^{th}$ vertex of the ${\ell}^{th}$ cycle of $G'$. Therefore, if there is an edge between $c_{j}^i$ and $c_{\ell}^k$, then $j \neq \ell$, $i = \ell$ and $k = j$. This implies then that if edges $e'' = (v'' = c_{j'}^{i'}, c_{j''}^{i''})$ and $e^! = (v'', c_{j^!}^{i^!})$ are in $G'$, then $i' = j'' = j^!$ and $j' = i'' = i^!$. This also implies that an edge between $v_{j'}$ and $v_{i'}$ exists in the original graph $G$. Since the algorithm only adds one edge between cycles for each edge in $G$, then $e''$ and $e^!$ must be the same edge.\\
\\
Thus, since any choice of two edges coming out of $v''$ that are not the neighboring cycle edges of $v''$ are the same, then it must be that there is only one other edge going out of $v''$, a contradiction. Thus, there exists no such vertex in $v''$ such that $\deg(v'') \neq 4$, which means $G'$ is a 4-regular graph.
\\
\end{proof}

\begin{lem} Vertex $s$ is connected to vertex $t$ in the graph of $G$ iff cycle $c_s$ is connected to cycle $c_t$ in the graph of $G'$.
\end{lem}
\begin{proof}
If $s$ and $t$ are connected in $G$, then there exists some path $P = p_1 \to p_2 \to ...  \to p_\ell$ connecting $s$ and $t$ such that $p_1 = s$, $p_\ell = t$ and, for all $1 \leq i \leq \ell-1$, there exists an edge $e = (p_i, p_{i+1}) \in E$. WLOG, let $P$ be the shortest such path between $s$ and $t$ in $G$, so there's no self-loops in $P$. Because there's an edge between $e = (p_i, p_{i+1})$ for all $1 \leq i \leq \ell-1$, then there must be an edge $e' =(c_{p_i}^{p_{i+1}}, c_{p_{i+1}}^{p_{i}}) \in G'$. Furthermore, we know we can build a path $P_j$ from any vertex in the cycle $c_{j}$ to any other vertex in the cycle $c_{j}$ as all of the vertices in the cycle are connected. Thus, we can construct a new path $P' = c_{p_1}^{p_{2}} \to  c_{p_{2}}^{p_{1}} \to P_{p_2} \to c_{p_2}^{p_{3}} \to  c_{p_{3}}^{p_{2}} \to P_{p_3} \to ... \to P_{p_{\ell-1}} \to c_{p_{\ell -1}}^{p_{\ell}} \to  c_{p_{\ell}}^{p_{\ell-1}}$. Thus, there exists a path $P'$ that connects cycle $c_{p_1} = c_{s}$ to cycle $c_{p_{\ell}} = c_{t}$.\\

On the other hand, if $s$ and $t$ are not connected, there won't be a path connecting $c_s$ to $c_t$. Suppose there were such a path $P'$ connecting $c_s$ to $c_t$. Then we could define another path $P$ wherein all edges connecting vertices in the same cycle were removed, and every edge that connected some vertex $c_i^j$ to some vertex $c_{i'}^{j'}$ such that $i \neq i'$ was replaced with an edge between vertices $(i, i')$ in the graph $G$. By the definition of the algorithm, if such an edge between vertices $c_i^j$ and $c_{i'}^{j'}$ exists in $G'$, then some edge exists between vertices $i$ and $i'$ exists in $G$. Therefore, $P'$ would be a valid path in $G$ from vertex $s$ to vertex $t$. However, no such path exists between $s$ and $t$ in $G$, which is a contradiction. Thus, our supposition that there exists a path between $c_s$ and $c_t$ must be false. 
\\
\end{proof}

\begin{lem} the graph reduction {\bf selfLooping4Regular}($G$) is polynomial time constructible.
\end{lem}
\begin{proof}
In the for loop at line 2, for each vertex in $G$, we add a cycle of $n$ vertices to $G'$, which has $n-1$ edges. Assuming it takes some constant number of steps to add a vertex and an edge to a graph, each cycle takes $O(n + n-1) = O(n)$ steps to add itself to $G'$. Since the for loop runs once for each vertex in $G$, then the total time complexity of this for loop is $O(n^2)$.\\

Next, in the for loop at line 5, we add an edge to $G'$ for each unique edge in $G$ that is not a self loop. There are no more than $n^2$ unique edges in an undirected graph. Again assuming a constant number of time-steps to add an edge, the loop has a time complexity of $O(n^2)$.\\

Lastly, for the for loop at line 9, we add self loops to each vertex in $G'$. We know for each vertex in $G$, we added a cycle of $n$ vertices to $G'$. Therefore, there are $n^2$ vertices in $G'$. Again assuming adding an edge is a constant time operation, the loop has a time complexity of $O(n^2)$.\\
\\
Thus, the reduction has a total time complexity of $O(3n^2) = O(n^2)$.
\\
\end{proof}



\begin{algorithm}[H]
\KwIn{An undirected 4-regular graph $G'$ with self loops at each vertex, a start vertex $s$, and an end vertex $t$}
\KwOut{{\bf Accept}, if $t$ is the ending vertex of the walk, otherwise output {\bf Reject}}
\textsf{currentVertex} = $s$\;
\For{$i=0, i < 10dn^3\log{n}, i++$} {
	randomly choose an $e$ from the set of edges going out of $\textsf{currentVertex}$\;
	Let $e = (\textsf{currentVertex}, \text{nextVertex})$\;
	\textsf{currentVertex} = \textsf{nextVertex}\;
}
\uIf{$\textsf{nextVertex}$ = $t$} {
{\bf Output}: {\bf Accept}\;
}
\Else {
{\bf Output}: {\bf Reject}\;
}

\caption{{\bf undirectedPath}($G'$, $s$, $t$)}
\end{algorithm}
\creturn
Before we begin our analysis, we must consider the following. Let $G$ be any arbitrary $n$ vertex graph. Let $A$ be the normalized adjacency matrix of $G$. Then $A$ is of the form
\[
\begin{bmatrix}
    \frac{\lvert E_{1,1}\rvert}{d_1}     &  \frac{\lvert E_{2,1}\rvert}{d_2} & \dots &  \frac{\lvert E_{n,1}\rvert}{d_n} \\
   \frac{\lvert E_{1,2}\rvert}{d_1}       &  \frac{\lvert E_{2,2}\rvert}{d_2}  & \dots &  \frac{\lvert E_{n,2}\rvert}{d_n}  \\
    \hdotsfor{4} \\
     \frac{\lvert E_{1,n}\rvert}{d_1}       & \frac{\lvert E_{2,n}\rvert}{d_2} & \dots &  \frac{\lvert E_{n,n}\rvert}{d_n} 
\end{bmatrix}
\]
where $E_{i,j}$ is the set of edges in $G$ that go from vertex $v_i$ to vertex $v_j$ and $d_\ell$ is the degree of the vertex $v_\ell$.

Notice that $\frac{\lvert E_{i,j}\rvert}{d_i} = A_{j,i}$ is the probability that, in a random walk, the next vertex in the walk is $v_j$ given that the current vertex of the walk is $v_i$. The $\ell^{th}$ column vector in $A$, $A^{\ell}$ is the probability vector for outcome of the next step of the random walk given that the current vertex is $v_{\ell}$. For all $\ell$,  the $L_1$-norm of $A^{\ell}$ is 1. To this end, $A$ is a Markov Chain encoding for the possible state outcomes of a random walk.\\

Now, consider the following operation. Let {\bf p} be a probability vector such that {\bf} is a positive column vector with an $L_1$-norm of 1. We multiply $A{\bf p}$ to get
\[
\begin{bmatrix}
    A_{1,1}    &  A_{1,2}  & \dots &  A_{1,n}  \\
   A_{2,1}       &  A_{2,2}   & \dots &  A_{2,n}   \\
    \hdotsfor{4} \\
     A_{n,1}        & A_{n,2}  & \dots &  A_{n,n} 
\end{bmatrix}
\begin{bmatrix}
    p_1 \\
   p_2 \\
    \hdotsfor{1} \\
     p_n \\
\end{bmatrix}
=
\begin{bmatrix}
    A_{1,1}p_1 + A_{1,2}p_2 + \dots + A_{1,n}p_n \\
    A_{2,1}p_1 + A_{2,2}p_2 + \dots + A_{2,n}p_n \\
    \hdotsfor{1} \\
      A_{n,1}p_1 + A_{n,2}p_2 + \dots + A_{n,n}p_n \\
\end{bmatrix}
=
{\bf q}
\]
If we let {\bf p} encode the probability vector for state of the current step of the walk such that for all $1 \leq i \leq n$ the $i^{th}$ coordinate of ${\bf p}$, ${\bf p}_i$, is the probability that the current vertex is $v_i$, then for all $1 \leq j \leq n$
\[
\begin{aligned}
{\bf q}_j &= \sum_{k =1}^nA_{j,k}{\bf p}_k\\
&= \sum_{k=1}^n \Pr\left(\begin{array}{l}
    \text{next vertex in the walk is }v_j \lvert \\
    \text{current vertex in the walk is }v_k 
  \end{array}\right)\Pr\Big(\text{current vertex in the walk is }v_k\Big)\\
&= \Pr(\text{next vertex in the walk is }v_j)
\end{aligned}
\]
Therefore, {\bf q} encodes the probability vector for the state of the next step of the walk. Now, if we take the operation $A{\bf q} = {\bf q}^2$, then ${\bf q}^2$ would clearly encode the probability vector for the state of the step after the next step of the walk, that is two steps after the current state of the walk encoded by ${\bf p}$. Therefore, ${\bf q}^i$, the probability vector for the state of the walk $i$ steps after the current step of the walk, is encoded by 
\[
{\bf q}^i = A{\bf q}^{i-1} = A^2{\bf q}^{i-2} = ... = A^{i-1}{\bf q} = A^i{\bf p}
\]

Let $\{{\bf e}^i\}_{i=1}^n$ represent the standard basis of $\mathbb{R}^n$ such that each ${\bf e}^i$ is an $n$-dimensional column vector with zero's in every coordinate except for the $i^{th}$ coordinate, which is 1. If we assume ${\bf e}^i$ to represent a probability vector for the state of a step of the random walk, then, during that step, we know the current vertex of the walk is guaranteed to be $v_i$. To this end, if ${\bf e}^i$ is the initial state of a random walk, then
\begin{equation}
A^T{\bf e}^i
\end{equation}
is the probability distribution of the state of a $T$ step random walk starting at vertex $v_i$ in an undirected graph $G$.\\

Now, for the following definitions, let $G = (V,E)$ be any $d$-regular undirected graph with $n$ vertices such that $n = \lvert V \rvert$ and  $A = A(G)$ is the normalized adjacency matrix of $G$. 
\begin{defn}
We define the vector ${\bf 1}$ to be the $n$ dimensional column vector with the value $1/n$ at every coordinate.
\end{defn}\creturn
Consider the vector 
\[
\begin{aligned}
A{\bf 1} &= \begin{bmatrix}
    A_{1,1}    &  A_{1,2}  & \dots &  A_{1,n}  \\
   A_{2,1}       &  A_{2,2}   & \dots &  A_{2,n}   \\
    \hdotsfor{4} \\
     A_{n,1}        & A_{n,2}  & \dots &  A_{n,n} 
\end{bmatrix}
\begin{bmatrix}
    1/n \\
   1/n \\
    \hdotsfor{1} \\
     1/n \\
\end{bmatrix}\\
&= 
\begin{bmatrix}
    A_{1,1}1/n + A_{1,2}1/n + \dots + A_{1,n}1/n \\
    A_{2,1}1/n + A_{2,2}1/n + \dots + A_{2,n}1/n \\
    \hdotsfor{1} \\
      A_{n,1}1/n + A_{n,2}1/n + \dots + A_{n,n}1/n \\
\end{bmatrix}.
\end{aligned}
\]
Since $G$ is a $d$-regular matrix, we know each $A_{i,j} =  \frac{\lvert E_{j,i}\rvert}{d}$. So,
\[
A{\bf 1} =
\begin{bmatrix}
    \frac{\lvert E_{1,1} \rvert +  \lvert E_{2,1} \rvert + ... + \lvert E_{n,1} \rvert  }{dn} \\
     \frac{\lvert E_{1,2} \rvert +  \lvert E_{2,2} \rvert + ... + \lvert E_{n,2} \rvert  }{dn} \\
    \hdotsfor{1} \\
       \frac{\lvert E_{1,n} \rvert +  \lvert E_{2,n} \rvert + ... + \lvert E_{n,n} \rvert  }{dn} \\
\end{bmatrix}.
\]
For all $1 \leq i \leq n$, $\lvert E_{1,i} \rvert +  \lvert E_{2,i} \rvert + ... + \lvert E_{n,i} \rvert = \sum_{j=1}^n \lvert E_{j,i} \rvert$. Again, since $G$ is a $d$-regular matrix, then we know
\[
\sum_{j=1}^n \lvert E_{j,i} \rvert = d.
\]
So,
\[
A{\bf 1} =
\begin{bmatrix}
    \frac{d}{dn} \\
     \frac{d}{dn} \\
    \hdotsfor{1} \\
       \frac{d}{dn} \\
\end{bmatrix}
=
{\bf 1}
\]
Thus, ${\bf 1}$ is an eigenvector of $A=A(G)$ with a corresponding eigenvalue of 1 for any $d$-regular undirected graph $G$ with $n$ vertices.

\begin{lem}
For any $d$-regular undirected graph $G$ with $n$ vertices, {\bf 1} is the eigenvector with the largest absolute valued eigenvalue of $A$, where $A$ is the normalized adjacency matrix of $G$.
\end{lem}
\begin{proof}
Suppose ${\bf v}$ is any eigenvector of  $A$ with eigenvalue $\lambda$. Then
\[
A{\bf v} = \lambda{\bf v},
\]
and therefore, for all $1 \leq i \leq n$
\[
\sum_{j = 1}^nA_{i, j}{\bf v}_j = \lambda{\bf v}_i. 
\]
Because ${\bf v}$ is an eigenvector of $A$, we know that ${\bf v}$ cannot be the 0 vector. Let the $k^{th}$ coordinate of ${\bf v}$ be a coordinate such that for all $1 \leq i \leq n$, $\lvert {\bf v}_i \rvert \leq \lvert {\bf v}_k \rvert$. We know ${\bf v}_k > 0$. So,
\[
\lambda = \sum_{j = 1}^nA_{k, j}\frac{{\bf v}_j}{{\bf v}_k},
\]
and therefore
\[\begin{aligned}
 \lvert \lambda \rvert &= \left| \sum_{j = 1}^nA_{k, j}\frac{{\bf v}_j}{{\bf v}_k} \right|\\
&\leq \sum_{j = 1}^n\left|A_{k, j}\frac{{\bf v}_j}{{\bf v}_k} \right|\\
 &= \sum_{j = 1}^n\bigg\lvert A_{k, j}\bigg\rvert \left| \frac{{\bf v}_j}{{\bf v}_k} \right|\\
 &\leq \sum_{j = 1}^n\bigg\lvert A_{k, j}\bigg\rvert.
\end{aligned}
\]
We already know $L_1$-norm for each column vector in $A$ is 1. Since $A$ is the adjacency matrix for an undirected graph, we know $A$ is symmetric. The $L_1$-norm of each row vector in $A$ must also then be 1. Thus, 
\[
\begin{aligned}
 \lvert \lambda \rvert &\leq \sum_{j = 1}^n\bigg\lvert A_{k, j}\bigg\rvert \\
 &= 1.
\end{aligned}
\]
\\
\end{proof}


For any $d$-regular undirected graph $G$ with $n$ vertices, we know that $A(G)$ is symmetric. Therefore, we can find an orthogonal basis of $n$ eigenvectors ${\bf v}^1, {\bf v}^2, ... ,{\bf v}^n$ with corresponding eigenvalues $\lambda_1, \lambda_2, ... , \lambda_n$. WLOG assume $\lvert \lambda_1 \rvert \geq \lvert \lambda_2 \rvert \geq ... \geq \lvert \lambda_n \rvert$. As a consequence of lemma 3.6, we know $\lambda_1 = 1$. Furthermore, there exists some eigenvectors ${\bf v}^2, {\bf v}^3, ..., {\bf v}^n$ of $A$ which are all mutually orthogonal with ${\bf 1}$.

\begin{lem}
For any $d$-regular undirected graph $G$ with $n$ vertices with normalized adjacency matrix $A(G) = A$, suppose vector ${\bf v} \perp {\bf 1}$. Then $A{\bf v} \perp {\bf 1}$.
\end{lem}
\begin{proof}
Let $A^\dagger$ be the transpose of $A$. Since $G$ is an undirected graph, we know that $A$ is a symmetric matrix. Therefore, $A^\dagger = A$. Now we take the dot product of ${\bf 1}$ and $A{\bf v}$ to show
\[
\begin{aligned}
\langle {\bf 1}, A{\bf v} \rangle &= \langle A^\dagger {\bf 1},{\bf v} \rangle\\
&= \langle A{\bf 1},{\bf v} \rangle\\
&= \langle {\bf 1},{\bf v} \rangle\\
&= 0.
\end{aligned}
\]
Since, $\langle {\bf 1}, A{\bf v} \rangle = 0$, it must be that $A{\bf v} \perp {\bf 1}$.
\\
\end{proof}

\begin{defn}
Let  $M$ be a normalized adjacency matrix, let ${\bf 1}^\perp$ be the set of vectors perpendicular to ${\bf 1}$ such that for any vector ${\bf v}^\perp \in {\bf 1}^\perp, \langle {\bf v}^\perp, {\bf 1} \rangle = 0$. For a vector ${\bf u}$ and for $p > 1$, let $\lvert \lvert {\bf u} \rvert \rvert_p$ denote the $L_p$-norm of ${\bf u}$. We define the parameter $\lambda(M)$ as the maximum value of $\lvert\lvert M{\bf v} \rvert\rvert_2$ over all vectors ${\bf v} \in {\bf 1}^\perp$ with $\lvert \lvert {\bf v} \rvert \rvert_2 = 1$.
\end{defn}\creturn

\begin{lem}
Let  $M$ be a normalized adjacency matrix. Then, for all ${\bf v} \perp {\bf 1}$, $\lvert \lvert M{\bf v} \rvert \rvert_2 \leq \lambda(M)\lvert\lvert {\bf v} \rvert\rvert_2$.
\end{lem}
\begin{proof}
Let ${\bf w} = \frac{{\bf v}}{\lvert \lvert {\bf v} \rvert \rvert _2}$. Then
\[
\begin{aligned}
\lvert \lvert {\bf w} \rvert \rvert_2 &= \left| \left| \frac{\bf{v}}{\lvert\lvert {\bf v} \rvert \rvert _2}\right| \right| _2\\
&= 1.
\end{aligned}
\]
By the definition 3.8 of $\lambda(M)$, we know that $\lvert \lvert A{\bf w} \rvert \rvert_2 \leq \lambda(M)$, so
\[
\begin{aligned}
\lambda(M) &\geq \lvert \lvert A{\bf w} \rvert \rvert_2\\
&= \left| \left| A \frac{{\bf v}}{\lvert \lvert {\bf v} \rvert \rvert _2} \right| \right| \\
&= \frac{1}{\lvert \lvert {\bf v} \rvert \rvert_2}\lvert \lvert A {\bf v} \rvert \rvert_2,
\end{aligned}
\]
and therefore
\[
 \lambda(M)\lvert\vert {\bf v} \rvert \rvert_2 \geq \lvert \lvert A {\bf v} \rvert \rvert_2.
\]
\end{proof}


%%%=== SECOND LARGEST EIGENVLUE LEM COMMENTED OUT ===%%%
\iffalse
\begin{lem}
$\lambda(A) = \lvert \lambda_2 \rvert$, where $\lambda_2$ is the second largest absolute valued eigenvalue of $A(G)$.
\end{lem}
\begin{proof}
hi
\end{proof}
\fi
%% ========================================================= %%


\begin{lem}
For any $d$-regular undirected graph $G$ with $n$ vertices and normalized adjacency matrix $A(G) = A$, let {\bf p} be any probability distribution vector over the vertices. Then
\[
\lvert\lvert A^T{\bf p}-{\bf 1}\rvert \rvert_2 \leq (\lambda(A))^T.
\]
\end{lem}
\begin{proof}
By lemma 3.9 we know for any vector ${\bf v} \perp {\bf 1}$,  we have $\lvert \lvert A{\bf v} \rvert \rvert_2 \leq \lambda(A)\lvert\lvert {\bf v} \rvert \rvert_2$. Furthermore, by lemma 3.7, we know that for any vector ${\bf v} \perp {\bf 1}$, we have $A{\bf v} \perp {\bf 1}$. So, for all $T' \geq 1$ we must have
\[
A^{T'}{\bf v} \perp {\bf 1}.
\]
Therefore, 
\[
\begin{aligned}
\lvert \lvert A^T {\bf v} \rvert \rvert _2 &= \lvert \lvert A (A^{T-1}{\bf  v}) \rvert \rvert _2 \leq \lambda(A)\lvert \lvert A^{T-1}{\bf  v} \rvert \rvert _2\\
&=\lambda(A)\lvert \lvert A (A^{T-2}{\bf  v}) \rvert \rvert _2 \leq \lambda(A)^2\lvert \lvert A^{T-2}{\bf  v} \rvert\rvert_2\\
& ...\\
&=\lambda(A)^{T-1}\lvert \lvert A{\bf  v} \rvert \rvert _2 \leq \lambda(A)^T\lvert \lvert {\bf  v} \rvert\rvert_2.\\
\end{aligned}
\]
Now, we can break the probability distribution vector  ${\bf p}$ into its components parallel and orthogonal to ${\bf 1}$ such that ${\bf p} = \alpha{1} + {\bf p}'$ where ${\bf p}' \perp {\bf 1}$. So,
\[
\begin{aligned}
0 &=  \langle {\bf 1}, {\bf p}' \rangle\\
&= \sum_{i=1}^n {\bf 1}_i{\bf p}'_i\\
&= \sum_{i=1}^n \frac{1}{n}{\bf p}'_i\\
&= \sum_{i=1}^n{\bf p}'_i.\\
 \end{aligned}
\]
Because we know ${\bf p}$ is a probability distribution, we know the sum of its coordinates $\sum_{i=1}^n {\bf p}_i = 1$. For all $1 \leq i \leq n$ each coordinate ${\bf p}_i = \alpha{\bf 1}_i + {\bf p}'_i = \frac{\alpha}{n} + {\bf p}'_i$. So,
\[
\begin{aligned}
1 &= \sum_{i = 1}^n \Big(\frac{\alpha}{n} + {\bf p}'_i \Big)\\
&=  \sum_{i = 1}^n \frac{\alpha}{n} + \sum_{i=1}^n {\bf p}'_i\\
&=  \alpha + \sum_{i=1}^n {\bf p}'_i\\
&= \alpha.\\
 \end{aligned}
\]
We can break down the operation $A^T{\bf p}$ into the components of ${\bf p}$ to show that
\[
A^T{\bf p} = A^T({\bf 1} + {\bf p}') = {\bf 1} + A^T{\bf p}',
\]
which implies that $A^T{\bf p} - {\bf 1} = A^T{\bf p}'$.\\

Because {\bf 1} and ${\bf p}'$ are orthogonal components of {\bf p}, by the pythagorean theorem, we have $\lvert \lvert {\bf p} \rvert \rvert_2 = \lvert \lvert {\bf 1 }\rvert \rvert_2 + \lvert \lvert {\bf p}'\rvert \rvert_2$, and therefore $\lvert \lvert {\bf p}' \rvert \rvert_2 \leq \lvert \lvert {\bf p} \rvert \rvert_2$. Then,
\[
\begin{aligned}
\sum_{i=1}^{n} {\bf p}_i^2 &\leq \sum_{i=1}^{n}{\bf p}_i\\
(\lvert\lvert {\bf p} \rvert \rvert_2)^2 &\leq 1\\ 
\sqrt{(\lvert\lvert {\bf p} \rvert \rvert_2)^2} &\leq \sqrt{1}\\
\lvert\lvert {\bf p} \rvert \rvert_2 &\leq  1,
\end{aligned}
\]
and therefore,
\[
\begin{aligned}
\lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_2 = \lvert \lvert A^T{\bf p}' \rvert \rvert_2 \leq \lambda(A)^T\lvert\lvert {\bf p}' \rvert \rvert \leq (\lambda(A))^T.
\end{aligned}
\]
\\
\end{proof}
\begin{lem}
For every $d$-regular undirected connected graph $G$ with $n$ vertices, self-loops at each vertex, and normalized-adjacency matrix $A$,  $\lambda(A) \leq 1-\frac{1}{8dn^3}$.
\end{lem}
\begin{proof}
Let ${\bf u}$ be any vector such that ${\bf u \perp {\bf 1}}$ and $\lvert \lvert {\bf u} \rvert \rvert_2 = 1$. Let ${\bf v} = A{\bf u}$. To prove this claim, we first show that $1 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2 = \sum_{i,j}A_{i,j}({\bf u}_i - {\bf v}_j)^2$.\\
\begin{equation}
\begin{aligned}
\sum_{i,j}A_{i,j}({\bf u}_i - {\bf v}_j)^2 &= \sum_{i=1}^{n}\sum_{j=1}^{n}A_{i,j}{\bf u}_i^2 - 2\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j + \sum_{j=1}^{n}\sum_{i=1}^{n}A_{i,j}{\bf v}_j^2\\
&= \sum_{i=1}^{n}{\bf u_i}^2\sum_{j=1}^{n}A_{i,j} - 2\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j + \sum_{j=1}^{n}{\bf v}_j^2\sum_{i=1}^{n}A_{i,j}\\
&= \sum_{i=1}^{n}{\bf u_i}^2 - 2\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j + \sum_{j=1}^{n}{\bf v}_j^2\\
&= (\lvert \lvert {\bf u} \rvert \rvert_2)^2 -  2\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j + (\lvert \lvert {\bf v} \rvert \rvert_2)^2.
\end{aligned}
\end{equation}
Now, if we expand $\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j$, we get
\[
\begin{aligned}
\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j &= \sum_{j=1}^n{\bf v}_j\sum_{i=1}^n A_{i,j}{\bf u}_i\\
&= \sum_{j=1}^n{\bf v}_jA^{\dagger}{\bf u}_j\\
&= \sum_{j=1}^n{\bf v}_jA{\bf u}_j\\
&= \langle {\bf v} , A{\bf u} \rangle\\
&= \langle {\bf v}, {\bf v} \rangle\\
&= (\lvert \lvert {\bf v} \rvert \rvert_2)^2.
\end{aligned}
\]
We substitute this result into equation 3.12 to get
\[
(\lvert \lvert {\bf u} \rvert \rvert_2)^2 -  2\sum_{i,j}A_{i,j}{\bf u}_i{\bf v}_j + (\lvert \lvert {\bf v} \rvert \rvert_2)^2 = (\lvert \lvert {\bf u} \rvert \rvert_2)^2 - 2(\lvert \lvert {\bf v} \rvert \rvert_2)^2 + (\lvert \lvert {\bf v} \rvert \rvert_2)^2 =  (\lvert \lvert {\bf u} \rvert \rvert_2)^2 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2.
\]
Given that $\lvert \lvert {\bf u} \rvert \rvert_2 = 1$,
\[
 (\lvert \lvert {\bf u} \rvert \rvert_2)^2 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2 =  1 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2.
\]\creturn
Now, we will show $1 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2 = \sum_{i,j}A_{i,j}({\bf u}_i - {\bf v}_j)^2 \geq \frac{1}{d4n^3}$. Because the sum $\sum_{i,j}A_{i,j}({\bf u}_i - {\bf v}_j)^2$ has no non-negative terms, if at least one $A_{i,j}({\bf u}_i - {\bf v}_j)^2$ term is greater than or equal to $\frac{1}{d4n^3}$ then we have sufficiently proven that $A_{i,j}({\bf u}_i - {\bf v}_j)^2 = 1 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2 \geq 1$.\\

Suppose for the sake of contradiction that for all $i,j$, $A_{i,j}({\bf u}_i - {\bf v}_j)^2 < \frac{1}{d4n^3}$. Because every vertex in $G$ has a self loop, we know that, for all $1 \leq i \leq n$, $A_{i,i} = 1$. So, in order for our supposition to hold, it must be that for all $i$,
$({\bf u}_i - {\bf v}_i)^2 < \frac{1}{4n^3}$, and therefore $\lvert {\bf u}_i - {\bf v}_i \lvert < \frac{1}{2n^{1.5}}$.\\

Let $f: [1:n] \to [1:n]$ be a function that maps some input $i$ to the index of the $i^{th}$ largest coordinate of ${\bf u}$. Because $\bf{u} \perp {\bf 1}$, we know $\langle {\bf u}, {\bf 1}\rangle = 0$, which means $\sum_{i=1}^n {\bf u}_i = 0$. Therefore, it must be that ${\bf u}_{f(1)} \geq 0 \geq {\bf u}_{f(n)}$. Furthermore, because $\lvert \lvert {\bf u} \rvert \rvert_2 = 1$, then we know some coordinate ${\bf u}_i^2 \geq \frac{1}{n}$. Thus, either ${\bf u}_{f(1)} \geq \frac{1}{\sqrt{n}}$ or ${\bf u}_{f(n)} \leq -\frac{1}{\sqrt{n}}$, which implies ${\bf u}_{f(1)}- {\bf u}_{f(n)} \geq \frac{1}{\sqrt{n}}$.\\

Now, because we know there are $n-1$ coordinates ordered between ${\bf u}_{f(1)}$ and ${\bf u}_{f(n)}$, we know there exists some $k$ such that  the difference between two of the consecutively ordered coordinates \[
\lvert {\bf u}_{f(k)} - {\bf u}_{f(k+1)} \rvert \geq\frac{1}{\sqrt{n}(n-1)} = \frac{1}{n^{1.5} - \sqrt{n}} \geq \frac{1}{n^{1.5}}.
\]

Now, we define the partition $S =\{f(1), ..., f(k)\}$ and let $\overline{S} = [n] \setminus S$. For any $i \in S$, $j \in \overline{S}$, it must hold that ${\bf u}_{i} - {\bf u}_{j} \geq \frac{1}{n^{1.5}}$. Because $G$ is connected, we know there exists some edge $(i', j')$ such that $f(i') \in S$ and $f(j') \in \overline{S}$.\\

By the triangle inequality,
\[
\lvert {\bf u}_{i'} - {\bf v}_{j'} \rvert \geq \lvert {\bf u}_{i'} - {\bf v}_{j'} \rvert - \lvert {\bf u}_{j'} - {\bf v}_{j'}\rvert.
\]
\creturn
Furthermore, by our earlier supposition, we assume $\lvert {\bf u}_{j'} - {\bf v}_{j'}\rvert < \frac{1}{2n^{1.5}}$. So,
\[
\begin{aligned}
\lvert {\bf u}_{i'} - {\bf v}_{j'} \rvert &\geq \lvert {\bf u}_{i'} - {\bf v}_{j'} \rvert - \lvert {\bf u}_{j'} - {\bf v}_{j'}\rvert\\
&\geq \lvert {\bf u}_{i'} - {\bf v}_{j'} \rvert - \frac{1}{2n^{1.5}}\\
&\geq \frac{1}{n^{1.5}}- \frac{1}{2n^{1.5}}\\
&= \frac{1}{2n^{1.5}}.\\
\end{aligned}
\]
Because there is an edge between vertices $v_{i'}$ and $v_{j'}$ in $G$, it must be that $A_{i', j'} \geq \frac{1}{d}$. Thus,
\[
A_{i',j'}({\bf u}_{i'} - {\bf v}_{j'})^2 \geq \frac{1}{d}(\frac{1}{2n^{1.5}})^2 = \frac{1}{d4n^3},
\]
which, finishing our argument by contradiction, implies
\[
\frac{1}{d4n^3} \leq  \sum_{i,j}A_{i,j}({\bf u}_i - {\bf v}_j)^2 = 1 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2.
\]
Since $\frac{1}{d4n^3} \leq 1 - (\lvert \lvert {\bf v} \rvert \rvert_2)^2$, we know
\[
(\lvert \lvert {\bf v} \rvert \rvert_2)^2 \leq 1- \frac{1}{d4n^3}.
\]
Now, consider
\[
(1-\frac{1}{d8n^3})^2 = 1 - \frac{1}{d4n^3} + \frac{1}{(d8n^3)^2}.
\]
It so follows that
\[
(\lvert \lvert {\bf v} \rvert \rvert_2)^2 \leq 1- \frac{1}{d4n^3} \leq (1-\frac{1}{d8n^3})^2, 
\]
 and therefore
 \[\lvert \lvert {\bf v} \rvert \rvert_2  \leq 1-\frac{1}{d8n^3}.
 \]
 \\

For all ${\bf u}' \perp {\bf 1}$ with an $L_2$-norm of one, $\lvert\lvert A{\bf u} \rvert\rvert_2 = \lambda(A)$. Therefore, because ${\bf u}$ is a vertex with ${\bf u} \perp {\bf 1}$ and $\lvert \lvert {\bf u} \rvert \rvert _2 = 1$, and we know $\lvert \lvert {\bf v} \rvert \rvert_2 = \lvert \lvert A{\bf u} \rvert \rvert_2 \leq 1-\frac{1}{d8n^3}$, it must hold that $\lambda(A) \leq 1-\frac{1}{d8n^3}$.
\\
\end{proof}


\begin{thm}
Let $G$ be any $d$-regular undirected graph with $n$ vertices such that each vertex in $G$ has a self-loop and normalized adjacency matrix $A$. Let $s$ be a vertex in $G$. Let $T > 14dn^3\log{n}$, $n \geq 16$,  and let $X^T$ denote  the probability distribution for the current vertex of the $T^{th}$ step in a random walk from $s$. Then, for every $j$ connected to $s$, $\Pr(X^T = j) \geq \frac{1}{2n}$ .
\end{thm}
\begin{proof}
Let ${\bf p}$ be any probability distribution vector. By lemma 3.10, we know that $\lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_2 \leq \lambda(A)^T \leq (1 - \frac{1}{d8n^3})^T$.\\
So, we take
\[
\begin{aligned}
(1 - \frac{1}{d8n^3})^T  &\leq (1 - \frac{1}{d8n^3})^{14dn^3\log{n}}\\
&= \exp\left(14dn^3\log{n}\log{\left(1 - \frac{1}{d8n^3}\right)}\right)
&= n^{-\frac{7}{4}}.
\end{aligned}
\]
Because $d > 1$ and $n \geq 1$, we can use $\log{(1 + x)} \leq x$ for $x > -1$ to show 
\[
\begin{aligned}
(1 - \frac{1}{d8n^3})^T  &\leq \exp\left(14dn^3\log(n)\log{\left(1 - \frac{1}{d8n^3}\right)}\right)\\
&\leq \exp\left(14dn^3\frac{-1}{d8n^3}\log{n}\right)\\
&= \exp\left(\log{n^{-\frac{7}{4}}}\right).\\
\end{aligned}
\]
Since $n \geq 16$,
\[\frac{1}{n^{\frac{7}{4}}} \leq \frac{1}{2n^{1.5}}.\]

Now, let vector ${\bf u}$ be an $n$ coordinate vector with the term $\frac{1}{\sqrt{n}}$ in every coordinate. Since $\frac{1}{2} + \frac{1}{2} = 1$, we will apply H\"older's inequality for $p = q = 2$ to ${\bf u}$ and $(A^T{\bf p} - {\bf 1})$ to get that 
\[
\lvert\lvert (A^T{\bf p} - {\bf 1}){\bf u}\rvert\rvert_1\leq \lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2} \lvert \lvert {\bf u}\rvert \rvert_{2},
\]
which we can expand to show that 
\[\begin{aligned}
\lvert\lvert (A^T{\bf p} - {\bf 1}){\bf u}\rvert\rvert_1 &= \sum_{i=1}^n \Big\lvert (A^T{\bf p} - {\bf 1})_i{\bf u}_i \Big\rvert\\
&= \frac{1}{\sqrt{n}}\sum_{i=1}^n \Big\lvert (A^T{\bf p} - {\bf 1})_i\Big\rvert\\
&= \frac{1}{\sqrt{n}}\lvert\lvert A^T{\bf p} - {\bf 1}\rvert\rvert_1 \\
&\leq \lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2} \lvert \lvert {\bf u}\rvert \rvert_{2}\\
&= \lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2} \sqrt{\sum_{i=1}^n {\bf u}_i^2} \\
&=  \lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2} \sqrt{\frac{1}{n}\sum_{i=1}^n 1}\\
&=  \lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2}.
\end{aligned}
\]
Thus, $\frac{1}{\sqrt{n}}\lvert\lvert A^T{\bf p} - {\bf 1}\rvert\rvert_1 \leq \lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2}$. Since $\lvert \lvert A^T{\bf p} - {\bf 1} \rvert \rvert_{2} \leq \frac{1}{2n^{1.5}}$, it then follows that $\lvert\lvert A^T{\bf p} - {\bf 1}\rvert\rvert_1 \leq \frac{1}{2n}$.\\

Because $\lvert\lvert A^T{\bf p} - {\bf 1}\rvert\rvert_1  = \sum_{i=1}^n \lvert (A^T{\bf p} - {\bf 1})_i\rvert \leq \frac{1}{2n}$, then we know for all $j$ connected to our start vertex $s$ that 
\[
\lvert( A^T{\bf p})_j - {\bf 1}_j \rvert = \lvert( A^T{\bf p})_j - \frac{1}{n} \rvert \leq \frac{1}{2n},
\]
and therefore
\[
\frac{1}{n} - (A^T{\bf p})_j \leq \frac{1}{2n},
\]
which implies that $(A^T{\bf p})_j \geq \frac{1}{2n}$. The $j^{th}$ coordinate of $A^T{\bf p}$ represents the probability that the walk is then currently at vertex $j$ during the $T^{th}$ step of the random walk. So, it follows that for all $j$ connected to $s$, $\Pr[X_T = j] \geq \frac{1}{2n}$.
\\
\end{proof}



\begin{cor}
For an undirected $d$-regular graph $G = (V, E)$ with $n = \lvert V \rvert \geq 16$, given any $0 < \delta \leq 1$, {\bf undirectedPath}($G$, $s$, $t$) will output {\bf Accept} if vertices $s$ and $t$ in $G$ are connected in time $O(n^4 \log{n})$ with probability $p > 1 - \delta$ after running a number $k > -2n\log(\delta)$ iterations of {\bf undirectedPath}($G$, $s$, $t$). 
\end{cor}
\begin{proof}
Let $X_i$ be a random variable that indicates whether {\bf undirectedPath}($G$, $s$, $t$) outputs {\bf Accept} on some $i^{th}$ iteration of running the algorithm. Let $X = \sum_{i = 1}^{k} X_i$. Then ${\bf Acccept}$ is returned by one of the $k$ iterations of the algorithm iff $X \geq 1$. Therefore, the probability that the algorithm accepts over $k$ iterations is
\[
\Pr(X \geq 1)  = 1 - \Pr(X  = 0) = 1 - \Pr\left(\bigcap_{i=1}^k X_i = 0 \right).
\]
Because each iteration of {\bf undirectedPath} is independent of the others and all have identical probability of ending on vertex $t$ an outputting {\bf Accept},
\[
\Pr\left(\bigcap_{i=1}^k X_i = 0 \right) = \prod_{i  = 0}^k \Pr(X_i = 0) = \Big(\Pr(X_j = 0)\Big)^k,
\]
for any $1 \leq j \leq k$. We know by theorem 3.13, that the probability $p^*$ that $t$ is the final vertex in the walk of {\bf undirectedPath}($G$, $s$, $t$) given that $s$ and $t$ are connected is $p^* \geq \frac{1}{2n}$. So, the probability $p'$ that $t$ is the ending vertex of the walk is $p' \leq 1- \frac{1}{2n}$. Therefore,
\[
\Big(\Pr(X_j = 0)\Big)^k \leq \Big(1- \frac{1}{2n}\Big)^k,
\]
which we can rewrite as
\[
\Bigg( \bigg( 1 - \frac{1}{2n}\bigg)^{2n}\Bigg)^\frac{k}{2n}.
\]

For all  $m > 1$,  we have $0 < (1 - \frac{1}{m})^m \leq \frac{1}{e}$. So, 
\begin{equation}\begin{aligned}
\Big(\Pr(X_j = 0)\Big)^k &\leq \Bigg( \bigg( 1 - \frac{1}{2n}\bigg)^{2n}\Bigg)^\frac{k}{2n}\\
& \leq \bigg( \frac{1}{e}\bigg)^{\frac{k}{2n}}\\
&= \exp\bigg(-\frac{k}{2n}\bigg).
\end{aligned}
\end{equation}

Now, substituting $k \geq -2n\log(\delta)$, we have
\[\begin{aligned}
\Big(\Pr(X_j = 0)\Big)^k &\leq  \exp\bigg(-\frac{k}{2n}\bigg)\\
&\leq \exp\Big(\log(\delta)\Big)\\
& = \delta.
\end{aligned}
\]

Thus, when $k \geq -2n\log(\delta)$, for any $0 < \delta \leq 1$,
\[
\Pr(X \geq 1)  = 1 - \Big(\Pr(X_j = 0)\Big)^k \geq 1 - \delta.
\]

Because each run {\bf undirectedPath}($G$, $s$, $t$) uses $O(n^3 \log{n})$ steps, then running $-2n\log(\delta)$ trials of {\bf undirectedPath}($G$, $s$, $t$) runs in $O(-2n\log(\delta) \cdot n^3 \log{n}) = O\Big(2n\log\Big(\frac{1}{\delta}\Big) \cdot n^3 \log{n}\Big) = O(n^4 \log{n})$ time.
\\
\end{proof}

%%%%%%% =============== LAST COROLLARY IS WRONG =============== %%%%%%%
\iffalse
\begin{cor}
For an undirected $d$-regular graph $G$ with $n \geq 16$ vertices and self-loops at each vertex, {\bf undirectedPath}($G$, $s$, $t$) can predict with $>99\%$ certainty whether two vertices $s$ and $t$ in $G$ are connected in time $O(n^4 \log{n})$. 
\end{cor}
\begin{proof}
From the proof of theorem 3.13, we know by running a single instance of {\bf undirectedPath}($G$, $s$, $t$), the probability that the walk ends on vertex $t$, given that $t$ is connected to $s$, is at least $\frac{1}{2n}$. We can easily calculate the variance of running a single trial of {\bf undirectedPath}($G$, $s$, $t$). Let $X_0$ be an indicator random variable that indicates whether or not when running a single instance of {\bf undirectedPath}($G$, $s$, $t$), the algorithm outputs {\bf Accept}. Because the only values that $X_0$ takes are $0$ and $1$, clearly $\E[X_0^2] = \E[X_0] = \frac{1}{2n}$. Therefore,
\[
V[X_0] = \E[X_0^2] - (\E[X_0])^2 = \frac{1}{2n} - \left(\frac{1}{2n}\right)^2 = \frac{1}{2n} - \frac{1}{4n^2}.
\]
Now, let $X = (X_1 + X_2 + ... + X_k)$ be a random variable that gives the number of times {\bf undirectedPath}($G$, $s$, $t$) outputs success over the course of some number $k$ trials. Because expectation is linear $\E[X] = \frac{k}{2n}$. Furthermore, because all of the $X_i$ are independent trials, $V[X] = k(\frac{1}{2n} - \frac{1}{4n^2})$. Suppose $k  = 200n$. Then $\E[X] = \frac{200n}{2n} = 100$, and $V[X] = \frac{200n}{2n} - \frac{200n}{4n^2} = 100 - \frac{50}{n}$.\\
\\
We apply Chebyshev's inequality to show that
\[\begin{aligned}
\Pr(\lvert X - 100 \rvert \geq 100 ) &\leq \frac{V[X]}{10000}\\
&= \frac{100 - \frac{50}{n}}{10000}\\
&= \frac{1}{100} - \frac{1}{200n} \\
&\leq \frac{1}{100}.
\end{aligned}
\]
Since Chebyshev's inequality calculates $\Pr(\lvert X - 100 \rvert \geq 100 )$, which accounts for the extreme when $X \geq 200$ and $X = 0$, it suffices to say that since $\Pr(\lvert X - 100 \rvert \geq 100 ) \leq \frac{1}{100}$, then $\Pr(X - 100 \leq 100 ) \leq \frac{1}{100}$. Therefore, if $s$ and $t$ are connected, then one of the $200n$ trials of {\bf undirectedPath}($G$, $s$, $t$) should output {\bf Accept} with greater than or equal to 99\% probability.\\
\\
Because each run {\bf undirectedPath}($G$, $s$, $t$) uses $O(n^3 \log{n})$ steps, then running $200n$ trials of {\bf undirectedPath}($G$, $s$, $t$) runs in $O(200n \cdot n^3 \log{n}) = O(n^4 \log{n})$ time.
\\
\end{proof}
\fi
%%%%%%% =============== THIS IS JUST WRONG =============== %%%%%%%

\subsection*{Acknowledgments} I would like to thank my thesis advisor Andrew Drucker for supporting my research and cultivating my interest in the theory of algorithms.  I'd also like Eric Thoma for countless hours of conversations and advice, as well as for being a genuinely great friend; it's unlikely I'd even understand math without his help. I'd also like to thank Christopher Rey, Ann Shih, Zachary Levine, Caitlin Sheehan, Kim Vu, Kanisha Williams, and countless others for their unwavering support and belief in me, even when my own wavered.
\begin{thebibliography}{9}

\bibitem{randalg}
Motwani, Rajeev \& Raghavan, Prabhakar (1995). Randomized Algorithms (1st ed.). New York, NY: Cambridge University Press.

\bibitem{kargersalg}
Karger, David (1993). "Global Min-cuts in RNC and Other Ramifications of a Simple Mincut Algorithm". Proc. 4th Annual ACM-SIAM Symposium on Discrete Algorithms.

\bibitem{princetonpaper} Arora, Sanjeev, Princeton University, cited 2018: "Lecture 2: Karger's Min Cut Algorithm".
[Available online at https://www.cs.princeton.edu/courses/archive/fall13/cos521/lecnotes/lec2final.pdf.] 

\bibitem{unreleasedtextbook} Arora, Sanjeev \& Barak, Boaz. (2009). Computational Complexity: a Modern Approach. New York, NY: Cambridge University Press.

\end{thebibliography}

\end{document}

